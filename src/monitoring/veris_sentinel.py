#!/usr/bin/env python3
"""
Veris Sentinel - Autonomous Monitor/Test/Report Agent for Veris Memory

Provides continuous monitoring and testing of Veris Memory with:
- Health probes for all services (S1)
- Golden fact recall testing (S2) 
- Paraphrase robustness testing (S3)
- Metrics wiring validation (S4)
- Security RBAC testing (S5)
- Backup/restore validation (S6)
- Configuration drift detection (S7)
- Performance capacity testing (S8)
- Graph intent validation (S9)
- Content pipeline monitoring (S10)
"""

import asyncio
import json
import logging
import os
import time
import sqlite3
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union, AsyncIterator, Callable, Awaitable, AsyncContextManager
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import deque, defaultdict
import aiohttp
from aiohttp import web
from aiohttp.web import Request, Response
import aiohttp_cors
import uuid

# Import Phase 4 Sentinel checks and models
from .sentinel.checks.s3_paraphrase_robustness import ParaphraseRobustness as ParaphraseRobustnessAdvanced
from .sentinel.checks.s9_graph_intent import GraphIntentValidation as GraphIntentValidationAdvanced  
from .sentinel.checks.s10_content_pipeline import ContentPipelineMonitoring as ContentPipelineMonitoringAdvanced
from .sentinel.models import SentinelConfig as SentinelConfigAdvanced

# Optional imports for external integrations
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

logger = logging.getLogger(__name__)


def _convert_config_for_advanced_checks(config: 'SentinelConfig') -> SentinelConfigAdvanced:
    """Convert old SentinelConfig to new advanced SentinelConfig format."""
    return SentinelConfigAdvanced({
        "veris_memory_url": config.target_base_url,
        "check_interval_seconds": config.schedule_cadence_sec,
        "alert_threshold_failures": 3,
        "webhook_url": config.alert_webhook,
        "github_repo": config.github_repo
    })


@dataclass
class CheckResult:
    """Result of a single check execution."""
    check_id: str
    timestamp: datetime
    status: str  # "pass", "fail", "warn"
    latency_ms: float
    error_message: Optional[str] = None
    metrics: Dict[str, float] = None
    notes: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        result = asdict(self)
        result['timestamp'] = self.timestamp.isoformat()
        return result


@dataclass
class SentinelConfig:
    """
    Configuration for Veris Sentinel.

    Uses environment variables for Docker deployments:
    - TARGET_BASE_URL: Base URL for Veris Memory API (default: http://localhost:8000)
    - REDIS_URL: Redis connection URL (default: redis://localhost:6379)
    - QDRANT_URL: Qdrant connection URL (default: http://localhost:6333)
    - NEO4J_BOLT: Neo4j bolt URL (default: bolt://localhost:7687)
    - NEO4J_USER: Neo4j username (default: veris_ro)

    Falls back to localhost URLs for local development when environment variables are not set.
    """
    target_base_url: Optional[str] = None
    redis_url: Optional[str] = None
    qdrant_url: Optional[str] = None
    neo4j_bolt: Optional[str] = None
    neo4j_user: Optional[str] = None
    schedule_cadence_sec: int = 60
    max_jitter_pct: int = 20
    per_check_timeout_sec: int = 10
    cycle_budget_sec: int = 45
    max_parallel_checks: int = 4
    burst_test_timeout_sec: int = 30  # Timeout for burst testing operations
    alert_webhook: Optional[str] = None
    github_repo: Optional[str] = None

    def __post_init__(self):
        """Set defaults from environment variables if not specified."""
        # Set target_base_url from environment (Docker) or use localhost (local dev)
        # Note: Default port is 8000 to match context-store default port
        if self.target_base_url is None:
            self.target_base_url = os.getenv('TARGET_BASE_URL', 'http://localhost:8000')

        # Set redis_url from environment or use localhost
        if self.redis_url is None:
            self.redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')

        # Set qdrant_url from environment or use localhost
        if self.qdrant_url is None:
            self.qdrant_url = os.getenv('QDRANT_URL', 'http://localhost:6333')

        # Set neo4j_bolt from environment or use localhost
        if self.neo4j_bolt is None:
            self.neo4j_bolt = os.getenv('NEO4J_BOLT', 'bolt://localhost:7687')

        # Set neo4j_user from environment or use default
        if self.neo4j_user is None:
            self.neo4j_user = os.getenv('NEO4J_USER', 'veris_ro')

    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value like a dictionary.

        This method allows the config to be used like a dictionary
        for backward compatibility with checks that read from environment.
        """
        # Map common keys to actual attributes
        if key == 'veris_memory_url':
            return self.target_base_url
        elif key == 'api_url':
            return self.target_base_url
        elif key == 'qdrant_url':
            return self.qdrant_url
        elif key == 'neo4j_url' or key == 'neo4j_bolt':
            return self.neo4j_bolt
        elif key == 'redis_url':
            return self.redis_url

        # For S7/S8 configuration, read from environment
        elif key.startswith('s7_') or key.startswith('s8_'):
            env_key = key.upper()
            return os.getenv(env_key, default)

        # Try to get from object attributes
        return getattr(self, key, default)


class VerisHealthProbe:
    """S1: Health probes for live/ready endpoints."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute health probe check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                # Test liveness endpoint
                async with session.get(f"{self.config.target_base_url}/health/live") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Liveness check failed: HTTP {resp.status}"
                        )
                    
                    live_data = await resp.json()
                    if live_data.get("status") != "alive":
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Liveness status not 'alive': {live_data.get('status')}"
                        )
                
                # Test readiness endpoint
                async with session.get(f"{self.config.target_base_url}/health/ready") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S1-probes",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Readiness check failed: HTTP {resp.status}"
                        )
                    
                    ready_data = await resp.json()
                    
                    # Verify component statuses
                    components = ready_data.get("components", [])
                    for component in components:
                        status = component.get("status", "unknown")
                        name = component.get("name", "unknown")
                        
                        if name == "qdrant" and status not in ["ok", "healthy"]:
                            return CheckResult(
                                check_id="S1-probes",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"Qdrant not healthy: {status}"
                            )
                        elif name in ["redis", "neo4j"] and status not in ["ok", "healthy", "degraded"]:
                            return CheckResult(
                                check_id="S1-probes",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"{name} not healthy: {status}"
                            )
                
                latency_ms = (time.time() - start_time) * 1000
                return CheckResult(
                    check_id="S1-probes",
                    timestamp=datetime.utcnow(),
                    status="pass",
                    latency_ms=latency_ms,
                    metrics={"latency_ms": latency_ms, "status_bool": 1.0},
                    notes="All health endpoints responding correctly"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S1-probes",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Health check exception: {str(e)}"
            )


class GoldenFactRecall:
    """S2: Golden fact recall testing with natural questions."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        self.test_dataset = [
            {
                "kv": {"name": "Matt"},
                "questions": ["What's my name?", "Who am I?"],
                "expect_contains": "Matt"
            },
            {
                "kv": {"food": "spicy"},
                "questions": ["What kind of food do I like?", "What food preference do I have?"],
                "expect_contains": "spicy"
            },
            {
                "kv": {"location": "San Francisco"},
                "questions": ["Where do I live?", "What's my location?"],
                "expect_contains": "San Francisco"
            }
        ]
        
    async def run_check(self) -> CheckResult:
        """Execute golden fact recall check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                total_tests = 0
                passed_tests = 0
                
                for test_case in self.test_dataset:
                    # Store the fact
                    store_payload = {
                        "user_id": f"sentinel_test_{uuid.uuid4().hex[:8]}",
                        "content": json.dumps(test_case["kv"]),
                        "content_type": "fact",
                        "metadata": {"test_type": "golden_recall", "sentinel": True}
                    }
                    
                    async with session.post(
                        f"{self.config.target_base_url}/api/store_context",
                        json=store_payload
                    ) as resp:
                        if resp.status != 200:
                            return CheckResult(
                                check_id="S2-golden-fact-recall",
                                timestamp=datetime.utcnow(),
                                status="fail",
                                latency_ms=(time.time() - start_time) * 1000,
                                error_message=f"Failed to store fact: HTTP {resp.status}"
                            )
                    
                    # Test retrieval with each question
                    for question in test_case["questions"]:
                        total_tests += 1
                        
                        retrieve_payload = {
                            "user_id": store_payload["user_id"],
                            "query": question,
                            "max_results": 5
                        }
                        
                        async with session.post(
                            f"{self.config.target_base_url}/api/retrieve_context",
                            json=retrieve_payload
                        ) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                
                                # Check if expected content is in top result
                                memories = result.get("memories", [])
                                if memories and test_case["expect_contains"].lower() in memories[0].get("content", "").lower():
                                    passed_tests += 1
                
                # Calculate precision at 1
                p_at_1 = passed_tests / total_tests if total_tests > 0 else 0.0
                
                latency_ms = (time.time() - start_time) * 1000
                
                if p_at_1 >= 1.0:
                    status = "pass"
                elif p_at_1 >= 0.8:
                    status = "warn"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S2-golden-fact-recall",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=latency_ms,
                    metrics={"p_at_1": p_at_1, "mrr": p_at_1, "coverage": p_at_1},
                    notes=f"P@1: {p_at_1:.2f} ({passed_tests}/{total_tests} tests passed)"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S2-golden-fact-recall",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Golden fact recall exception: {str(e)}"
            )


class ParaphraseRobustness:
    """S3: Paraphrase robustness testing for semantic consistency."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        self.test_cases = [
            {
                "original": "What is my name?",
                "paraphrases": ["Who am I?", "Tell me my name", "What do you call me?"],
                "context": {"name": "Alice"}
            },
            {
                "original": "What are my preferences?", 
                "paraphrases": ["What do I like?", "Tell me my settings", "What are my choices?"],
                "context": {"preferences": "dark mode, notifications off"}
            }
        ]
    
    async def run_check(self) -> CheckResult:
        """Execute paraphrase robustness check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                total_tests = 0
                passed_tests = 0
                consistency_scores = []
                
                for test_case in self.test_cases:
                    # Store the context
                    store_payload = {
                        "context": test_case["context"],
                        "namespace": "sentinel_test"
                    }
                    
                    async with session.post(
                        f"{self.config.target_base_url}/api/store_context",
                        json=store_payload,
                        headers={'Content-Type': 'application/json'}
                    ) as store_resp:
                        if store_resp.status != 200:
                            continue
                    
                    # Test original query
                    original_response = await self._query_context(session, test_case["original"])
                    if not original_response:
                        continue
                    
                    # Test paraphrases
                    for paraphrase in test_case["paraphrases"]:
                        total_tests += 1
                        paraphrase_response = await self._query_context(session, paraphrase)
                        
                        if paraphrase_response:
                            # Calculate semantic consistency (simplified)
                            consistency = self._calculate_consistency(original_response, paraphrase_response)
                            consistency_scores.append(consistency)
                            
                            if consistency > 0.8:  # 80% threshold
                                passed_tests += 1
                
                if total_tests == 0:
                    return CheckResult(
                        check_id="S3-paraphrase-robustness",
                        timestamp=datetime.utcnow(),
                        status="fail",
                        latency_ms=(time.time() - start_time) * 1000,
                        error_message="No paraphrase tests could be executed"
                    )
                
                consistency_rate = passed_tests / total_tests
                avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
                
                status = "pass" if consistency_rate >= 0.8 else "warn" if consistency_rate >= 0.6 else "fail"
                
                return CheckResult(
                    check_id="S3-paraphrase-robustness",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=(time.time() - start_time) * 1000,
                    metrics={
                        "consistency_rate": consistency_rate,
                        "avg_consistency_score": avg_consistency,
                        "total_tests": total_tests,
                        "passed_tests": passed_tests
                    },
                    notes=f"Paraphrase consistency: {consistency_rate:.1%}, avg score: {avg_consistency:.3f}"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S3-paraphrase-robustness",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Paraphrase test error: {str(e)}"
            )
    
    async def _query_context(self, session: aiohttp.ClientSession, query: str) -> Optional[str]:
        """Query context with a specific question."""
        try:
            query_payload = {
                "query": query,
                "namespace": "sentinel_test"
            }
            
            async with session.post(
                f"{self.config.target_base_url}/api/retrieve_context",
                json=query_payload,
                headers={'Content-Type': 'application/json'}
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return data.get("result", "")
                return None
        except Exception:
            return None
    
    def _calculate_consistency(self, response1: str, response2: str) -> float:
        """Calculate semantic consistency between two responses (simplified)."""
        # Simple consistency check - in practice would use embeddings
        if not response1 or not response2:
            return 0.0
        
        # Basic keyword overlap
        words1 = set(response1.lower().split())
        words2 = set(response2.lower().split())
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0


class MetricsWiring:
    """S4: Metrics wiring validation."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute metrics wiring check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # Test dashboard endpoint for metrics
                async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S4-metrics-wiring",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Dashboard endpoint failed: HTTP {resp.status}"
                        )
                    
                    dashboard_data = await resp.json()
                    
                    # Verify required metrics are present
                    required_fields = ["system", "services", "timestamp"]
                    missing_fields = [field for field in required_fields if field not in dashboard_data]
                    
                    if missing_fields:
                        return CheckResult(
                            check_id="S4-metrics-wiring",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Missing dashboard fields: {missing_fields}"
                        )
                    
                    # Check for percentile metrics in analytics
                    try:
                        async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics") as analytics_resp:
                            if analytics_resp.status == 200:
                                analytics_data = await analytics_resp.json()
                                analytics_available = "analytics" in analytics_data
                            else:
                                analytics_available = False
                    except:
                        analytics_available = False
                    
                    latency_ms = (time.time() - start_time) * 1000
                    
                    return CheckResult(
                        check_id="S4-metrics-wiring",
                        timestamp=datetime.utcnow(),
                        status="pass",
                        latency_ms=latency_ms,
                        metrics={"labels_present": 1.0, "percentiles_present": 1.0 if analytics_available else 0.0},
                        notes=f"Dashboard metrics available, analytics: {analytics_available}"
                    )
                    
        except Exception as e:
            return CheckResult(
                check_id="S4-metrics-wiring",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Metrics wiring exception: {str(e)}"
            )


class SecurityNegatives:
    """S5: Security RBAC and WAF validation."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute security negatives check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                unauthorized_blocked = 0
                total_security_tests = 0
                
                # Test 1: Reader token should work for retrieve_context
                total_security_tests += 1
                headers = {"Authorization": "Bearer reader_token_placeholder"}
                
                retrieve_payload = {
                    "user_id": "security_test_user",
                    "query": "test query",
                    "max_results": 1
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/retrieve_context",
                    json=retrieve_payload,
                    headers=headers
                ) as resp:
                    # Should work (200) or fail auth (401/403), but not 500
                    if resp.status in [200, 401, 403]:
                        pass  # Expected behavior
                    else:
                        return CheckResult(
                            check_id="S5-security-negatives",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Unexpected status for reader retrieve: {resp.status}"
                        )
                
                # Test 2: Reader token should be blocked for store_context
                total_security_tests += 1
                store_payload = {
                    "user_id": "security_test_user",
                    "content": "test content",
                    "content_type": "test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload,
                    headers=headers
                ) as resp:
                    # Should be blocked (401/403)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                    elif resp.status == 200:
                        pass  # Might not have RBAC implemented yet
                    else:
                        return CheckResult(
                            check_id="S5-security-negatives",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Unexpected status for reader store: {resp.status}"
                        )
                
                # Test 3: Invalid/guest token should be blocked for admin endpoints
                total_security_tests += 1
                invalid_headers = {"Authorization": "Bearer invalid_guest_token"}
                
                async with session.get(
                    f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics",
                    headers=invalid_headers
                ) as resp:
                    # Should be blocked or work without auth (for now)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                    elif resp.status == 200:
                        pass  # Might not require auth yet
                
                # Test 4: No authentication header
                total_security_tests += 1
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload
                ) as resp:
                    # Should be blocked or work (depending on current auth implementation)
                    if resp.status in [401, 403]:
                        unauthorized_blocked += 1
                
                # Calculate security block rate
                unauthorized_block_rate = unauthorized_blocked / total_security_tests if total_security_tests > 0 else 0.0
                
                latency_ms = (time.time() - start_time) * 1000
                
                # For now, just check that endpoints are responding reasonably
                # In full implementation, would expect higher block rate
                if unauthorized_block_rate >= 0.0:  # Lenient for current implementation
                    status = "pass"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S5-security-negatives",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=latency_ms,
                    metrics={"unauthorized_block_rate": unauthorized_block_rate},
                    notes=f"Security tests: {unauthorized_blocked}/{total_security_tests} properly blocked"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S5-security-negatives",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Security test exception: {str(e)}"
            )


class BackupRestore:
    """S6: Backup and restore validation testing."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
    
    async def run_check(self) -> CheckResult:
        """Execute backup and restore validation check."""
        start_time = time.time()
        
        try:
            backup_tests = 0
            backup_passed = 0
            test_data = {"test_backup": f"backup_test_{int(time.time())}"}
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                # Test 1: Store test data for backup validation
                store_payload = {
                    "context": test_data,
                    "namespace": "backup_test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/store_context",
                    json=store_payload,
                    headers={'Content-Type': 'application/json'}
                ) as resp:
                    if resp.status == 200:
                        backup_tests += 1
                        backup_passed += 1
                
                # Test 2: Verify data persistence (simulates backup integrity)
                retrieve_payload = {
                    "query": "backup_test",
                    "namespace": "backup_test"
                }
                
                async with session.post(
                    f"{self.config.target_base_url}/api/retrieve_context",
                    json=retrieve_payload,
                    headers={'Content-Type': 'application/json'}
                ) as resp:
                    backup_tests += 1
                    if resp.status == 200:
                        data = await resp.json()
                        if test_data["test_backup"] in str(data):
                            backup_passed += 1
                
                # Test 3: Check if backup endpoints exist (if implemented)
                try:
                    async with session.get(f"{self.config.target_base_url}/api/backup/status") as resp:
                        backup_tests += 1
                        if resp.status in [200, 404]:  # 404 is acceptable if not implemented
                            backup_passed += 1
                except Exception:
                    pass  # Backup API may not be implemented
                
                backup_success_rate = backup_passed / backup_tests if backup_tests > 0 else 0.0
                
                if backup_tests == 0:
                    status = "warn"
                    notes = "No backup tests could be executed"
                elif backup_success_rate >= 0.8:
                    status = "pass"
                    notes = f"Backup validation passed ({backup_passed}/{backup_tests} tests)"
                elif backup_success_rate >= 0.5:
                    status = "warn"
                    notes = f"Backup validation partially successful ({backup_passed}/{backup_tests} tests)"
                else:
                    status = "fail"
                    notes = f"Backup validation failed ({backup_passed}/{backup_tests} tests)"
                
                return CheckResult(
                    check_id="S6-backup-restore",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=(time.time() - start_time) * 1000,
                    metrics={
                        "backup_tests": backup_tests,
                        "backup_passed": backup_passed,
                        "success_rate": backup_success_rate
                    },
                    notes=notes
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S6-backup-restore",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Backup validation error: {str(e)}"
            )


class ConfigParity:
    """S7: Configuration drift detection."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute configuration parity check."""
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # Test dashboard for configuration indicators
                async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard") as resp:
                    if resp.status != 200:
                        return CheckResult(
                            check_id="S7-config-parity",
                            timestamp=datetime.utcnow(),
                            status="fail",
                            latency_ms=(time.time() - start_time) * 1000,
                            error_message=f"Cannot access dashboard for config check: HTTP {resp.status}"
                        )
                    
                    dashboard_data = await resp.json()
                    
                    # Check for expected configuration indicators
                    services = dashboard_data.get("services", [])
                    qdrant_found = any(s.get("name") == "Qdrant" for s in services)
                    
                    # Basic configuration checks
                    config_ok = True
                    config_issues = []
                    
                    if not qdrant_found:
                        config_issues.append("Qdrant service not found in dashboard")
                        config_ok = False
                    
                    # Check if analytics endpoint responds (indicates Phase 2 deployment)
                    try:
                        async with session.get(f"{self.config.target_base_url.replace(':8000', ':8080')}/api/dashboard/analytics") as analytics_resp:
                            analytics_available = analytics_resp.status == 200
                    except:
                        analytics_available = False
                    
                    if not analytics_available:
                        config_issues.append("Analytics endpoint not available")
                    
                    latency_ms = (time.time() - start_time) * 1000
                    
                    return CheckResult(
                        check_id="S7-config-parity",
                        timestamp=datetime.utcnow(),
                        status="pass" if config_ok else "warn",
                        latency_ms=latency_ms,
                        metrics={
                            "qdrant_found": 1.0 if qdrant_found else 0.0,
                            "analytics_available": 1.0 if analytics_available else 0.0
                        },
                        notes=f"Config issues: {'; '.join(config_issues) if config_issues else 'None'}"
                    )
                    
        except Exception as e:
            return CheckResult(
                check_id="S7-config-parity",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Config parity check exception: {str(e)}"
            )


class CapacitySmoke:
    """S8: Short burst performance testing."""
    
    def __init__(self, config: SentinelConfig) -> None:
        self.config: SentinelConfig = config
        
    async def run_check(self) -> CheckResult:
        """Execute capacity smoke test."""
        start_time = time.time()
        
        try:
            # Simple burst test - 20 concurrent requests
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                tasks = []
                
                for i in range(20):
                    task = self._single_request(session, i)
                    tasks.append(task)
                
                # Execute burst test with timeout protection
                try:
                    results = await asyncio.wait_for(
                        asyncio.gather(*tasks, return_exceptions=True),
                        timeout=self.config.burst_test_timeout_sec
                    )
                except asyncio.TimeoutError:
                    logger.warning(f"Burst test timeout after {self.config.burst_test_timeout_sec}s")
                    results = [Exception("Burst test timeout") for _ in tasks]
                
                # Analyze results
                successful_requests = 0
                failed_requests = 0
                response_times = []
                
                for result in results:
                    if isinstance(result, Exception):
                        failed_requests += 1
                    else:
                        status, response_time = result
                        if status:
                            successful_requests += 1
                            response_times.append(response_time)
                        else:
                            failed_requests += 1
                
                # Calculate metrics
                error_rate = failed_requests / len(results) if results else 1.0
                p95_ms = sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0
                p99_ms = sorted(response_times)[int(len(response_times) * 0.99)] if response_times else 0
                
                total_latency_ms = (time.time() - start_time) * 1000
                
                # Evaluate against thresholds
                if p95_ms <= 300 and error_rate <= 0.005:
                    status = "pass"
                elif p95_ms <= 500 and error_rate <= 0.01:
                    status = "warn"
                else:
                    status = "fail"
                
                return CheckResult(
                    check_id="S8-capacity-smoke",
                    timestamp=datetime.utcnow(),
                    status=status,
                    latency_ms=total_latency_ms,
                    metrics={"p95_ms": p95_ms, "p99_ms": p99_ms, "error_rate": error_rate},
                    notes=f"Burst test: {successful_requests}/{len(results)} successful, P95: {p95_ms:.1f}ms"
                )
                
        except Exception as e:
            return CheckResult(
                check_id="S8-capacity-smoke",
                timestamp=datetime.utcnow(),
                status="fail",
                latency_ms=(time.time() - start_time) * 1000,
                error_message=f"Capacity smoke test exception: {str(e)}"
            )
    
    async def _single_request(self, session: aiohttp.ClientSession, request_id: int) -> Tuple[bool, float]:
        """Execute a single test request."""
        start_time = time.time()
        
        try:
            async with session.get(f"{self.config.target_base_url}/health/live") as resp:
                response_time = (time.time() - start_time) * 1000
                return resp.status == 200, response_time
        except:
            response_time = (time.time() - start_time) * 1000
            return False, response_time




class SentinelRunner:
    """Main Veris Sentinel runner with scheduling and reporting."""
    
    def __init__(self, config: SentinelConfig, db_path: str = "/var/lib/sentinel/sentinel.db") -> None:
        self.config: SentinelConfig = config
        self.db_path: str = db_path
        self.running: bool = False
        
        # Convert config for advanced checks
        advanced_config = _convert_config_for_advanced_checks(config)
        
        # Initialize checks - using advanced Phase 4 implementations
        self.checks: Dict[str, Union[VerisHealthProbe, GoldenFactRecall, ParaphraseRobustness, MetricsWiring, SecurityNegatives, BackupRestore, ConfigParity, CapacitySmoke, GraphIntentValidationAdvanced, ContentPipelineMonitoringAdvanced]] = {
            "S1-probes": VerisHealthProbe(config),
            "S2-golden-fact-recall": GoldenFactRecall(config),
            "S3-paraphrase-robustness": ParaphraseRobustnessAdvanced(advanced_config),
            "S4-metrics-wiring": MetricsWiring(config),
            "S5-security-negatives": SecurityNegatives(config),
            "S6-backup-restore": BackupRestore(config),
            "S7-config-parity": ConfigParity(config),
            "S8-capacity-smoke": CapacitySmoke(config),
            "S9-graph-intent": GraphIntentValidationAdvanced(advanced_config),
            "S10-content-pipeline": ContentPipelineMonitoringAdvanced(advanced_config)
        }
        
        # Ring buffers for data retention
        self.failures: deque = deque(maxlen=200)
        self.reports: deque = deque(maxlen=50)
        self.traces: deque = deque(maxlen=500)
        
        # External service resilience tracking
        self.webhook_failures: int = 0
        self.github_failures: int = 0
        self.webhook_circuit_open: bool = False
        self.github_circuit_open: bool = False
        self.last_webhook_attempt: Optional[datetime] = None
        self.last_github_attempt: Optional[datetime] = None
        
        # Initialize database
        self._init_database()
    
    def _is_webhook_circuit_open(self) -> bool:
        """Check if webhook circuit breaker is open."""
        if not self.webhook_circuit_open:
            return False
        
        # Reset circuit after 5 minutes
        if (self.last_webhook_attempt and 
            datetime.utcnow() - self.last_webhook_attempt > timedelta(minutes=5)):
            logger.info("Resetting webhook circuit breaker")
            self.webhook_circuit_open = False
            self.webhook_failures = 0
            return False
        
        return True
    
    def _is_github_circuit_open(self) -> bool:
        """Check if GitHub API circuit breaker is open."""
        if not self.github_circuit_open:
            return False
        
        # Reset circuit after 10 minutes
        if (self.last_github_attempt and 
            datetime.utcnow() - self.last_github_attempt > timedelta(minutes=10)):
            logger.info("Resetting GitHub API circuit breaker")
            self.github_circuit_open = False
            self.github_failures = 0
            return False
        
        return True
    
    async def _send_webhook_with_retry(self, alert_data: Dict[str, Any]) -> bool:
        """Send webhook alert with retry logic and circuit breaker."""
        if self._is_webhook_circuit_open():
            logger.debug("Webhook circuit breaker is open, skipping alert")
            return False
        
        max_retries = 3
        base_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                self.last_webhook_attempt = datetime.utcnow()
                
                # Use exponential backoff
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    logger.debug(f"Webhook retry {attempt} after {delay}s delay")
                    await asyncio.sleep(delay)
                
                # Send webhook (in thread pool to avoid blocking)
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    lambda: requests.post(
                        self.config.alert_webhook, 
                        json=alert_data, 
                        timeout=10,
                        headers={'Content-Type': 'application/json'}
                    )
                )
                
                # Success - reset failure count
                self.webhook_failures = 0
                logger.debug("Webhook alert sent successfully")
                return True
                
            except requests.exceptions.Timeout:
                logger.warning(f"Webhook timeout on attempt {attempt + 1}")
            except requests.exceptions.ConnectionError:
                logger.warning(f"Webhook connection error on attempt {attempt + 1}")
            except requests.exceptions.RequestException as e:
                logger.warning(f"Webhook request error on attempt {attempt + 1}: {e}")
            except Exception as e:
                logger.error(f"Unexpected webhook error on attempt {attempt + 1}: {e}")
                break  # Don't retry on unexpected errors
        
        # All retries failed
        self.webhook_failures += 1
        if self.webhook_failures >= 5:
            logger.error("Opening webhook circuit breaker after 5 failures")
            self.webhook_circuit_open = True
        
        return False
    
    async def _create_github_issue_with_retry(self, issue_data: Dict[str, Any]) -> bool:
        """Create GitHub issue with retry logic and circuit breaker."""
        if not self.config.github_repo or self._is_github_circuit_open():
            return False
        
        max_retries = 2
        base_delay = 2.0
        
        for attempt in range(max_retries):
            try:
                self.last_github_attempt = datetime.utcnow()
                
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    logger.debug(f"GitHub API retry {attempt} after {delay}s delay")
                    await asyncio.sleep(delay)
                
                # Create GitHub issue (in thread pool)
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: requests.post(
                        f"https://api.github.com/repos/{self.config.github_repo}/issues",
                        json=issue_data,
                        timeout=15,
                        headers={
                            'Accept': 'application/vnd.github.v3+json',
                            'Authorization': f'token {os.environ.get("GITHUB_TOKEN", "")}',
                            'Content-Type': 'application/json'
                        }
                    )
                )
                
                if response.status_code == 201:
                    self.github_failures = 0
                    logger.info("GitHub issue created successfully")
                    return True
                else:
                    logger.warning(f"GitHub API returned status {response.status_code}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"GitHub API timeout on attempt {attempt + 1}")
            except requests.exceptions.ConnectionError:
                logger.warning(f"GitHub API connection error on attempt {attempt + 1}")
            except requests.exceptions.RequestException as e:
                logger.warning(f"GitHub API request error on attempt {attempt + 1}: {e}")
            except Exception as e:
                logger.error(f"Unexpected GitHub API error on attempt {attempt + 1}: {e}")
                break
        
        # All retries failed
        self.github_failures += 1
        if self.github_failures >= 3:
            logger.error("Opening GitHub API circuit breaker after 3 failures")
            self.github_circuit_open = True
        
        return False
    
    def _init_database(self) -> None:
        """Initialize SQLite database for persistence."""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS check_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    check_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    status TEXT NOT NULL,
                    latency_ms REAL NOT NULL,
                    error_message TEXT,
                    metrics TEXT,
                    notes TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cycle_reports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_checks INTEGER NOT NULL,
                    passed_checks INTEGER NOT NULL,
                    failed_checks INTEGER NOT NULL,
                    cycle_duration_ms REAL NOT NULL,
                    report_data TEXT
                )
            """)
    
    async def run_single_cycle(self) -> Dict[str, Any]:
        """Run a single monitoring cycle."""
        cycle_start = time.time()
        cycle_id = uuid.uuid4().hex[:8]
        
        logger.info(f"Starting monitoring cycle {cycle_id}")
        
        # Run checks with limited concurrency
        semaphore = asyncio.Semaphore(self.config.max_parallel_checks)
        tasks = []
        
        for check_id, check_instance in self.checks.items():
            task = self._run_single_check(semaphore, check_instance, check_id)
            tasks.append(task)
        
        # Execute all checks with timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=self.config.cycle_budget_sec
            )
        except asyncio.TimeoutError:
            logger.error(f"Cycle {cycle_id} exceeded budget of {self.config.cycle_budget_sec}s")
            results = [CheckResult("timeout", datetime.utcnow(), "fail", 0, "Cycle timeout")]
        
        # Process results
        cycle_results = []
        passed_checks = 0
        failed_checks = 0
        
        for result in results:
            if isinstance(result, Exception):
                cycle_results.append(CheckResult(
                    "exception", datetime.utcnow(), "fail", 0, str(result)
                ))
                failed_checks += 1
            else:
                cycle_results.append(result)
                if result.status == "pass":
                    passed_checks += 1
                else:
                    failed_checks += 1
                    self.failures.append(result)
        
        cycle_duration_ms = (time.time() - cycle_start) * 1000
        
        # Create cycle report
        cycle_report = {
            "cycle_id": cycle_id,
            "timestamp": datetime.utcnow().isoformat(),
            "total_checks": len(cycle_results),
            "passed_checks": passed_checks,
            "failed_checks": failed_checks,
            "cycle_duration_ms": cycle_duration_ms,
            "results": [result.to_dict() for result in cycle_results]
        }
        
        self.reports.append(cycle_report)
        
        # Store in database
        self._store_cycle_results(cycle_results, cycle_report)
        
        logger.info(f"Cycle {cycle_id} complete: {passed_checks}/{len(cycle_results)} passed")
        
        return cycle_report
    
    async def _run_single_check(self, semaphore: asyncio.Semaphore, 
                               check_instance, check_id: str) -> CheckResult:
        """Run a single check with semaphore limiting."""
        async with semaphore:
            try:
                return await asyncio.wait_for(
                    check_instance.run_check(),
                    timeout=self.config.per_check_timeout_sec
                )
            except asyncio.TimeoutError:
                return CheckResult(
                    check_id, datetime.utcnow(), "fail", 
                    self.config.per_check_timeout_sec * 1000,
                    f"Check timeout after {self.config.per_check_timeout_sec}s"
                )
            except Exception as e:
                return CheckResult(
                    check_id, datetime.utcnow(), "fail", 0, 
                    f"Check exception: {str(e)}"
                )
    
    def _store_cycle_results(self, results: List[CheckResult], cycle_report: Dict[str, Any]) -> None:
        """Store cycle results in database."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # Store individual check results
                for result in results:
                    conn.execute("""
                        INSERT INTO check_results 
                        (check_id, timestamp, status, latency_ms, error_message, metrics, notes)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        result.check_id,
                        result.timestamp.isoformat(),
                        result.status,
                        result.latency_ms,
                        result.error_message,
                        json.dumps(result.metrics) if result.metrics else None,
                        result.notes
                    ))
                
                # Store cycle report
                conn.execute("""
                    INSERT INTO cycle_reports
                    (timestamp, total_checks, passed_checks, failed_checks, cycle_duration_ms, report_data)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    cycle_report["timestamp"],
                    cycle_report["total_checks"],
                    cycle_report["passed_checks"],
                    cycle_report["failed_checks"],
                    cycle_report["cycle_duration_ms"],
                    json.dumps(cycle_report)
                ))
        except Exception as e:
            logger.error(f"Failed to store cycle results: {e}")
    
    async def start_scheduler(self) -> None:
        """Start the continuous monitoring scheduler."""
        self.running = True
        logger.info("Starting Veris Sentinel scheduler")
        
        while self.running:
            try:
                # Add jitter to prevent thundering herd
                jitter = random.uniform(
                    -self.config.max_jitter_pct / 100,
                    self.config.max_jitter_pct / 100
                ) * self.config.schedule_cadence_sec
                
                sleep_time = self.config.schedule_cadence_sec + jitter
                
                # Run monitoring cycle
                cycle_report = await self.run_single_cycle()
                
                # Check for critical failures and alert
                if cycle_report["failed_checks"] > 0:
                    await self._handle_failures(cycle_report)
                
                # Sleep until next cycle
                await asyncio.sleep(max(1, sleep_time))
                
            except Exception as e:
                logger.error(f"Scheduler error: {e}")
                await asyncio.sleep(30)  # Recovery delay
    
    async def _handle_failures(self, cycle_report: Dict[str, Any]) -> None:
        """Handle failures with alerting."""
        failed_results = [
            result for result in cycle_report["results"]
            if result["status"] in ["fail", "warn"]
        ]
        
        for result in failed_results:
            logger.error(f"Check {result['check_id']} failed: {result.get('error_message', 'Unknown error')}")
            
            # Send webhook alert if configured (with resilience)
            if self.config.alert_webhook and REQUESTS_AVAILABLE:
                alert_data = {
                    "text": f" Veris Sentinel Alert: {result['check_id']} failed",
                    "attachments": [{
                        "color": "danger",
                        "fields": [
                            {"title": "Check", "value": result['check_id'], "short": True},
                            {"title": "Status", "value": result['status'], "short": True},
                            {"title": "Error", "value": result.get('error_message', 'Unknown'), "short": False},
                            {"title": "Latency", "value": f"{result['latency_ms']:.1f}ms", "short": True},
                            {"title": "Timestamp", "value": datetime.utcnow().isoformat(), "short": True}
                        ]
                    }]
                }
                
                webhook_success = await self._send_webhook_with_retry(alert_data)
                if not webhook_success:
                    logger.warning(f"Failed to send webhook alert for {result['check_id']} after retries")
            
            # Create GitHub issue for critical failures (with resilience)
            if (result['status'] == 'fail' and 
                self.config.github_repo and 
                REQUESTS_AVAILABLE):
                
                issue_data = {
                    "title": f"Veris Sentinel Critical Failure: {result['check_id']}",
                    "body": f"""
**Check ID**: {result['check_id']}
**Status**: {result['status']}
**Error**: {result.get('error_message', 'Unknown error')}
**Latency**: {result['latency_ms']:.1f}ms
**Timestamp**: {datetime.utcnow().isoformat()}

**Cycle Report**:
- Total Checks: {cycle_report['total_checks']}
- Failed Checks: {cycle_report['failed_checks']}
- Cycle Duration: {cycle_report['cycle_duration_ms']:.1f}ms

This issue was automatically created by Veris Sentinel monitoring.
""",
                    "labels": ["bug", "monitoring", "sentinel", "critical"]
                }
                
                github_success = await self._create_github_issue_with_retry(issue_data)
                if github_success:
                    logger.info(f"Created GitHub issue for critical failure: {result['check_id']}")
                else:
                    logger.warning(f"Failed to create GitHub issue for {result['check_id']}")
    
    def stop(self) -> None:
        """Stop the scheduler."""
        self.running = False
        logger.info("Stopping Veris Sentinel scheduler")
    
    def get_status(self) -> Dict[str, Any]:
        """Get current status and last cycle summary."""
        last_report = self.reports[-1] if self.reports else None
        
        return {
            "running": self.running,
            "last_cycle": last_report,
            "total_cycles": len(self.reports),
            "failure_count": len(self.failures),
            "config": asdict(self.config)
        }


# HTTP API Server for Sentinel
from aiohttp import web, web_request
import aiohttp_cors

# Import rate limiter
try:
    from ..core.rate_limiter import get_rate_limiter
except ImportError:
    import sys
    import os
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from src.core.rate_limiter import get_rate_limiter


async def sentinel_rate_limit_middleware(
    request: web.Request, 
    handler: Callable[[web.Request], Awaitable[web.Response]]
) -> web.Response:
    """Rate limiting middleware for Sentinel API endpoints."""
    # Define rate limits for Sentinel endpoints
    endpoint_limits = {
        '/status': {'rpm': 300, 'burst': 50},      # 5 req/sec, burst 50
        '/run': {'rpm': 12, 'burst': 3},           # 0.2 req/sec, burst 3 (expensive)
        '/checks': {'rpm': 60, 'burst': 10},       # 1 req/sec, burst 10
        '/metrics': {'rpm': 180, 'burst': 30},     # 3 req/sec, burst 30
        '/report': {'rpm': 60, 'burst': 10},       # 1 req/sec, burst 10
    }
    
    # Extract client information
    client_info = {
        'remote_addr': request.remote,
        'user_agent': request.headers.get('User-Agent', ''),
        'client_id': request.headers.get('X-Client-ID', '')
    }
    
    rate_limiter = get_rate_limiter()
    client_id = rate_limiter.get_client_id(client_info)
    
    # Map endpoint path to rate limit key
    endpoint_path = request.path
    if endpoint_path in endpoint_limits:
        # Add Sentinel endpoint limits to rate limiter using thread-safe method
        endpoint_key = f"sentinel{endpoint_path}"
        rate_limiter.register_endpoint_limit(
            endpoint_key, 
            endpoint_limits[endpoint_path]["rpm"], 
            endpoint_limits[endpoint_path]["burst"]
        )
        
        try:
            # Check rate limits
            allowed, error_msg = await rate_limiter.check_rate_limit(
                endpoint_key, client_id, 1
            )
            
            if not allowed:
                logger.warning(f"Rate limit exceeded for {client_id} on {endpoint_path}: {error_msg}")
                return web.json_response({
                    "error": "Rate limit exceeded",
                    "message": error_msg,
                    "endpoint": endpoint_path,
                    "timestamp": datetime.utcnow().isoformat()
                }, status=429, headers={
                    "Retry-After": "60",
                    "X-RateLimit-Endpoint": endpoint_key
                })
            
            # Check burst protection
            burst_ok, burst_msg = await rate_limiter.check_burst_protection(client_id)
            if not burst_ok:
                logger.warning(f"Burst protection triggered for {client_id} on {endpoint_path}: {burst_msg}")
                return web.json_response({
                    "error": "Too many requests",
                    "message": burst_msg,
                    "endpoint": endpoint_path,
                    "timestamp": datetime.utcnow().isoformat()
                }, status=429, headers={
                    "Retry-After": "10",
                    "X-RateLimit-Type": "burst"
                })
        
        except Exception as e:
            logger.error(f"Rate limiting error for {endpoint_path}: {e}")
            # Continue to endpoint on rate limiting errors
    
    # Call the handler
    response = await handler(request)
    
    # Add rate limit headers to response
    if endpoint_path in endpoint_limits:
        endpoint_key = f"sentinel{endpoint_path}"
        response.headers["X-RateLimit-Endpoint"] = endpoint_key
        response.headers["X-RateLimit-Client"] = client_id[:8]  # Truncated for privacy
    
    return response


class SentinelAPI:
    """HTTP API server for Veris Sentinel."""
    
    def __init__(self, sentinel: SentinelRunner, port: int = 9090) -> None:
        self.sentinel: SentinelRunner = sentinel
        self.port: int = port

        # Storage for host-based check results
        self.host_check_results: Dict[str, CheckResult] = {}

        # Create app with rate limiting middleware
        middlewares = [sentinel_rate_limit_middleware]
        self.app: web.Application = web.Application(middlewares=middlewares)

        self._setup_routes()
        logger.info(" Sentinel API rate limiting enabled")
    
    def _setup_routes(self) -> None:
        """Setup HTTP API routes."""
        self.app.router.add_get('/status', self.status_handler)
        self.app.router.add_post('/run', self.run_handler)
        self.app.router.add_get('/checks', self.checks_handler)
        self.app.router.add_get('/metrics', self.metrics_handler)
        self.app.router.add_get('/report', self.report_handler)
        self.app.router.add_get('/rate-limit-status', self.rate_limit_status_handler)
        self.app.router.add_post('/host-checks/firewall', self.host_firewall_check_handler)
        
        # Enable CORS
        cors = aiohttp_cors.setup(self.app, defaults={
            "*": aiohttp_cors.ResourceOptions(
                allow_credentials=True,
                expose_headers="*",
                allow_headers="*",
                allow_methods="*"
            )
        })
        
        for route in list(self.app.router.routes()):
            cors.add(route)
    
    async def status_handler(self, request: Request) -> Response:
        """Handle /status endpoint."""
        status = self.sentinel.get_status()
        return web.json_response(status)
    
    async def run_handler(self, request: Request) -> Response:
        """Handle /run endpoint - trigger immediate cycle."""
        try:
            cycle_report = await self.sentinel.run_single_cycle()
            return web.json_response({
                "success": True,
                "cycle_report": cycle_report
            })
        except Exception as e:
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)
    
    async def checks_handler(self, request: Request) -> Response:
        """Handle /checks endpoint - list available checks."""
        checks = {
            check_id: {
                "description": check_instance.__class__.__doc__ or "No description",
                "enabled": True
            }
            for check_id, check_instance in self.sentinel.checks.items()
        }
        return web.json_response({"checks": checks})
    
    async def metrics_handler(self, request: Request) -> Response:
        """Handle /metrics endpoint - Prometheus-style metrics."""
        # Simple text-based metrics for now
        metrics_text = "# Veris Sentinel Metrics\n"
        
        if self.sentinel.reports:
            last_report = self.sentinel.reports[-1]
            metrics_text += f"sentinel_checks_total {last_report['total_checks']}\n"
            metrics_text += f"sentinel_checks_passed {last_report['passed_checks']}\n"
            metrics_text += f"sentinel_checks_failed {last_report['failed_checks']}\n"
            metrics_text += f"sentinel_cycle_duration_ms {last_report['cycle_duration_ms']}\n"
        
        metrics_text += f"sentinel_failure_buffer_size {len(self.sentinel.failures)}\n"
        metrics_text += f"sentinel_running {1 if self.sentinel.running else 0}\n"
        
        return web.Response(text=metrics_text, content_type="text/plain")
    
    async def report_handler(self, request: Request) -> Response:
        """Handle /report endpoint - JSON report of last N cycles."""
        n = int(request.query.get('n', 10))
        reports = list(self.sentinel.reports)[-n:]
        
        return web.json_response({
            "reports": reports,
            "total_reports": len(self.sentinel.reports),
            "failure_count": len(self.sentinel.failures)
        })
    
    async def rate_limit_status_handler(self, request: Request) -> Response:
        """Handle /rate-limit-status endpoint - get rate limit status."""
        try:
            # Extract client information
            client_info = {
                'remote_addr': request.remote,
                'user_agent': request.headers.get('User-Agent', ''),
                'client_id': request.headers.get('X-Client-ID', '')
            }
            
            rate_limiter = get_rate_limiter()
            client_id = rate_limiter.get_client_id(client_info)
            
            # Get status for all Sentinel endpoints
            endpoint_statuses = {}
            sentinel_endpoints = ['/status', '/run', '/checks', '/metrics', '/report']
            
            for endpoint_path in sentinel_endpoints:
                endpoint_key = f"sentinel{endpoint_path}"
                if endpoint_key in rate_limiter.endpoint_limits:
                    status = rate_limiter.get_rate_limit_info(endpoint_key, client_id)
                    endpoint_statuses[endpoint_path] = status
            
            return web.json_response({
                "success": True,
                "client_id": client_id,
                "timestamp": datetime.utcnow().isoformat(),
                "endpoint_statuses": endpoint_statuses,
                "rate_limiting": {
                    "enabled": True,
                    "global_burst_protection": True
                }
            })
        
        except Exception as e:
            logger.error(f"Failed to get rate limit status: {e}")
            return web.json_response({
                "success": False,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }, status=500)

    async def host_firewall_check_handler(self, request: Request) -> Response:
        """
        Handle /host-checks/firewall endpoint - receive firewall status from host.

        This endpoint receives firewall check results from the host-based monitoring script
        since Docker containers cannot directly check the host's UFW firewall status.

        Request body should be a CheckResult JSON from the host script.
        Requires X-Host-Secret header for authentication.
        """
        try:
            # Authenticate the request with shared secret
            # SECURITY: No default secret - must be explicitly configured
            expected_secret = os.getenv('HOST_CHECK_SECRET')

            # Reject insecure default from older deployments
            if expected_secret == 'veris_host_check_default_secret_change_me':
                logger.error("SECURITY: HOST_CHECK_SECRET is set to insecure default value - rejecting authentication")
                return web.json_response({
                    "success": False,
                    "error": "Server misconfiguration: insecure secret detected"
                }, status=500)

            if not expected_secret:
                logger.error("SECURITY: HOST_CHECK_SECRET environment variable not set - authentication unavailable")
                return web.json_response({
                    "success": False,
                    "error": "Server misconfiguration: authentication not configured"
                }, status=500)

            provided_secret = request.headers.get('X-Host-Secret')

            if not provided_secret:
                logger.warning("Host check request missing X-Host-Secret header")
                return web.json_response({
                    "success": False,
                    "error": "Authentication required: X-Host-Secret header missing"
                }, status=401)

            if provided_secret != expected_secret:
                logger.warning(f"Host check request with invalid secret from {request.remote}")
                return web.json_response({
                    "success": False,
                    "error": "Invalid authentication credentials"
                }, status=403)

            # Parse the incoming check result
            data = await request.json()

            # Validate required fields
            required_fields = ['check_id', 'timestamp', 'status', 'latency_ms', 'message']
            missing_fields = [f for f in required_fields if f not in data]

            if missing_fields:
                return web.json_response({
                    "success": False,
                    "error": f"Missing required fields: {', '.join(missing_fields)}"
                }, status=400)

            # Convert timestamp string to datetime if needed with proper validation
            if isinstance(data.get('timestamp'), str):
                try:
                    # Handle various timestamp formats
                    timestamp_str = data['timestamp']

                    # Remove 'Z' suffix and replace with UTC timezone
                    if timestamp_str.endswith('Z'):
                        timestamp_str = timestamp_str.replace('Z', '+00:00')

                    # Parse ISO format timestamp
                    data['timestamp'] = datetime.fromisoformat(timestamp_str)

                    # Ensure we have a timezone-aware datetime
                    if data['timestamp'].tzinfo is None:
                        # If naive, assume UTC
                        from datetime import timezone
                        data['timestamp'] = data['timestamp'].replace(tzinfo=timezone.utc)

                    # Validate timestamp is not too far in the future or past
                    now = datetime.now(data['timestamp'].tzinfo)
                    time_diff = abs((data['timestamp'] - now).total_seconds())
                    if time_diff > 3600:  # More than 1 hour difference
                        logger.warning(f"Host check timestamp differs from server time by {time_diff}s")

                except (ValueError, AttributeError) as e:
                    return web.json_response({
                        "success": False,
                        "error": f"Invalid timestamp format: {data.get('timestamp')}. Expected ISO format (e.g., '2025-11-06T12:34:56Z' or '2025-11-06T12:34:56+00:00')"
                    }, status=400)

            # Create CheckResult object
            check_result = CheckResult(
                check_id=data['check_id'],
                timestamp=data['timestamp'],
                status=data['status'],
                latency_ms=data['latency_ms'],
                message=data['message'],
                details=data.get('details', {})
            )

            # Store the result
            self.host_check_results[check_result.check_id] = check_result

            # Log the result
            logger.info(f"Received host check: {check_result.check_id} [{check_result.status}] - {check_result.message}")

            # If status is fail or warn, also add to sentinel failures for alerting
            if check_result.status in ["fail", "warn"]:
                self.sentinel.failures.append(check_result)

            return web.json_response({
                "success": True,
                "message": "Host check result received and stored",
                "check_id": check_result.check_id,
                "status": check_result.status,
                "timestamp": check_result.timestamp.isoformat()
            })

        except json.JSONDecodeError:
            return web.json_response({
                "success": False,
                "error": "Invalid JSON in request body"
            }, status=400)
        except Exception as e:
            logger.error(f"Failed to process host firewall check: {e}")
            return web.json_response({
                "success": False,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }, status=500)

    def get_host_check_result(self, check_id: str) -> Optional['CheckResult']:
        """
        Get the most recent result for a host-based check.

        Args:
            check_id: The check identifier (e.g., "S11-firewall-status")

        Returns:
            The most recent CheckResult for this check, or None if not found
        """
        return self.host_check_results.get(check_id)

    async def start_server(self) -> None:
        """Start the HTTP API server."""
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, '0.0.0.0', self.port)
        await site.start()
        logger.info(f"Sentinel API server started on port {self.port}")