#!/usr/bin/env python3
"""
Unified Dashboard System for Veris Memory

Provides dual-format dashboard output:
- JSON format for AI agent consumption
- ASCII format for human operator visibility
"""

import json
import time
import asyncio
import logging
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from dataclasses import dataclass, asdict
from pathlib import Path

# Check psutil availability at module level
HAS_PSUTIL = False
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    psutil = None
    logging.getLogger(__name__).warning(
        "psutil not available - system metrics will use fallback values. "
        "Install psutil for accurate system monitoring: pip install psutil"
    )

# Import existing monitoring components with improved fallback handling
try:
    from ..core.monitoring import MCPMetrics
    from .metrics_collector import MetricsCollector, HealthChecker
except ImportError:
    # Fallback imports for testing and standalone execution
    import sys
    import os
    
    # Add project root to path if not already there
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    
    try:
        from src.core.monitoring import MCPMetrics
        from src.monitoring.metrics_collector import MetricsCollector, HealthChecker
    except ImportError:
        # Create mock components if monitoring modules don't exist
        class MockMCPMetrics:
            def __init__(self):
                pass
        
        class MockMetricsCollector:
            def __init__(self):
                pass
            def start_collection(self):
                pass
            def stop_collection(self):
                pass
            def get_metric_stats(self, name, minutes):
                return {'count': 0}
            def get_metric_value(self, name, labels=None):
                return 0
        
        class MockHealthChecker:
            def __init__(self, metrics_collector):
                pass
            def run_checks(self):
                return {}
        
        MCPMetrics = MockMCPMetrics
        MetricsCollector = MockMetricsCollector
        HealthChecker = MockHealthChecker

logger = logging.getLogger(__name__)


@dataclass
class SystemMetrics:
    """System-level metrics"""
    cpu_percent: float
    memory_total_gb: float
    memory_used_gb: float
    memory_percent: float
    disk_total_gb: float
    disk_used_gb: float
    disk_percent: float
    load_average: List[float]
    uptime_hours: float


@dataclass
class ServiceMetrics:
    """Service health metrics"""
    name: str
    status: str
    port: int
    uptime_hours: Optional[float] = None
    memory_mb: Optional[float] = None
    operations_per_sec: Optional[int] = None
    connections: Optional[int] = None
    custom_metrics: Optional[Dict[str, Any]] = None


@dataclass
class VerisMetrics:
    """Veris Memory specific metrics"""
    total_memories: int
    memories_today: int
    avg_query_latency_ms: float
    p99_latency_ms: float
    error_rate_percent: float
    active_agents: int
    successful_operations_24h: int
    failed_operations_24h: int


@dataclass
class SecurityMetrics:
    """Security and compliance metrics"""
    failed_auth_attempts: int
    blocked_ips: int
    waf_blocks_today: int
    ssl_cert_expiry_days: int
    rbac_violations: int
    audit_events_24h: int


@dataclass
class BackupMetrics:
    """Backup and disaster recovery metrics"""
    last_backup_time: datetime
    backup_size_gb: float
    restore_tested: bool
    last_restore_time_seconds: Optional[float]
    backup_success_rate_percent: float
    offsite_sync_status: str


class UnifiedDashboard:
    """
    Unified dashboard system providing both JSON and ASCII output formats
    for comprehensive Veris Memory monitoring.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        """Initialize unified dashboard with configuration."""
        self.config = config or self._get_default_config()
        self.metrics_collector = MetricsCollector()
        self.health_checker = HealthChecker(self.metrics_collector)
        self.mcp_metrics = MCPMetrics()
        
        # Don't auto-start collection - use our own async collection loop instead
        
        # Dashboard state
        self.last_update = None
        self.cached_metrics = None
        self.cache_duration = self.config.get('cache_duration_seconds', 30)
        self._collection_running = False
        self._collection_task = None
        
        # Store service clients for health checks
        self.service_clients = {
            'neo4j': None,
            'qdrant': None,
            'redis': None
        }
        
        logger.info("ðŸŽ¯ UnifiedDashboard initialized")

    def _get_default_config(self) -> Dict[str, Any]:
        """Get default dashboard configuration."""
        return {
            'refresh_interval_seconds': 5,
            'cache_duration_seconds': 30,
            'collection_interval_seconds': 30,
            'ascii': {
                'width': 80,
                'use_color': True,
                'use_emoji': True,
                'progress_bar_width': 10
            },
            'json': {
                'pretty_print': True,
                'include_internal_timing': True,
                'include_trends': True
            },
            'thresholds': {
                'memory_warning_percent': 80,
                'memory_critical_percent': 95,
                'disk_warning_percent': 85,
                'disk_critical_percent': 95,
                'cpu_warning_percent': 80,
                'cpu_critical_percent': 95,
                'error_rate_warning_percent': 1.0,
                'error_rate_critical_percent': 5.0,
                'latency_warning_ms': 100,
                'latency_critical_ms': 500
            },
            'fallback_data': {
                'system': {
                    'memory_total_gb': 64.0,
                    'memory_used_gb': 22.0,
                    'cpu_percent': 0.0,
                    'memory_percent': 34.4,
                    'disk_total_gb': 436.0,
                    'disk_used_gb': 13.0,
                    'disk_percent': 3.0,
                    'load_average': [0.23, 0.24, 0.11],
                    'uptime_hours': 247.0
                },
                'veris': {
                    'total_memories': 84532,
                    'memories_today': 1247,
                    'avg_query_latency_ms': 23.0,
                    'p99_latency_ms': 89.0,
                    'error_rate_percent': 0.02,
                    'active_agents': 12,
                    'successful_operations_24h': 15420,
                    'failed_operations_24h': 3
                },
                'security': {
                    'failed_auth_attempts': 0,
                    'blocked_ips': 2,
                    'waf_blocks_today': 7,
                    'ssl_cert_expiry_days': 87,
                    'rbac_violations': 0,
                    'audit_events_24h': 245
                },
                'backup': {
                    'backup_size_gb': 4.7,
                    'restore_tested': True,
                    'last_restore_time_seconds': 142.0,
                    'backup_success_rate_percent': 100.0,
                    'offsite_sync_status': 'healthy'
                }
            }
        }

    def set_service_clients(self, neo4j_client: Optional[Any] = None, 
                           qdrant_client: Optional[Any] = None, 
                           redis_client: Optional[Any] = None) -> None:
        """Set service clients for real health checks."""
        if neo4j_client:
            self.service_clients['neo4j'] = neo4j_client
        if qdrant_client:
            self.service_clients['qdrant'] = qdrant_client
        if redis_client:
            self.service_clients['redis'] = redis_client
            
    async def start_collection_loop(self) -> None:
        """Start background collection loop."""
        if self._collection_running:
            logger.warning("Collection loop already running")
            return
            
        self._collection_running = True
        self._collection_task = asyncio.create_task(self._collection_loop())
        logger.info("ðŸ”„ Dashboard collection loop started")
        
    async def stop_collection_loop(self) -> None:
        """Stop background collection loop."""
        self._collection_running = False
        if self._collection_task:
            self._collection_task.cancel()
            try:
                await self._collection_task
            except asyncio.CancelledError:
                pass
        logger.info("â¹ï¸ Dashboard collection loop stopped")
        
    async def _collection_loop(self) -> None:
        """Background collection loop with enhanced error handling."""
        interval = self.config.get('collection_interval_seconds', 30)
        consecutive_failures = 0
        max_failures = 5
        
        while self._collection_running:
            try:
                await self.collect_all_metrics(force_refresh=True)
                consecutive_failures = 0  # Reset on success
                logger.debug(f"Metrics collected at {datetime.utcnow()}")
            except asyncio.CancelledError:
                logger.info("Collection loop cancelled")
                break
            except ConnectionError as e:
                consecutive_failures += 1
                logger.warning(f"Connection error in collection loop (attempt {consecutive_failures}): {e}")
                if consecutive_failures >= max_failures:
                    logger.error(f"Too many consecutive connection failures ({consecutive_failures}), stopping collection")
                    self._collection_running = False
                    break
            except Exception as e:
                consecutive_failures += 1
                logger.error(f"Unexpected error in collection loop (attempt {consecutive_failures}): {e}", exc_info=True)
                if consecutive_failures >= max_failures:
                    logger.error(f"Too many consecutive failures ({consecutive_failures}), stopping collection")
                    self._collection_running = False
                    break
                
            # Wait for next collection with exponential backoff on failures
            try:
                sleep_interval = interval
                if consecutive_failures > 0:
                    # Exponential backoff with jitter: 30s, 60s, 120s, etc.
                    base_backoff = min(interval * (2 ** consecutive_failures), 300)  # Max 5 minutes
                    # Add Â±25% jitter to prevent thundering herd
                    jitter_factor = 0.75 + (0.5 * random.random())  # Random between 0.75 and 1.25
                    sleep_interval = base_backoff * jitter_factor
                    logger.info(f"Using backoff interval: {sleep_interval:.1f}s (base: {base_backoff}s, jitter: {jitter_factor:.2f}) due to {consecutive_failures} failures")
                    
                await asyncio.sleep(sleep_interval)
            except asyncio.CancelledError:
                logger.info("Collection loop sleep cancelled")
                break

    async def collect_all_metrics(self, force_refresh: bool = False) -> Dict[str, Any]:
        """
        Collect comprehensive system metrics from all sources.
        
        Args:
            force_refresh: Force refresh even if cache is valid
            
        Returns:
            Dictionary containing all collected metrics
        """
        # Check cache validity
        if (not force_refresh and self.cached_metrics and self.last_update and
            (datetime.utcnow() - self.last_update).total_seconds() < self.cache_duration):
            return self.cached_metrics

        try:
            # Collect metrics in parallel
            system_task = asyncio.create_task(self._collect_system_metrics())
            service_task = asyncio.create_task(self._collect_service_metrics())
            veris_task = asyncio.create_task(self._collect_veris_metrics())
            security_task = asyncio.create_task(self._collect_security_metrics())
            backup_task = asyncio.create_task(self._collect_backup_metrics())

            # Wait for all collections to complete
            system_metrics, service_metrics, veris_metrics, security_metrics, backup_metrics = await asyncio.gather(
                system_task, service_task, veris_task, security_task, backup_task,
                return_exceptions=True
            )

            # Handle exceptions gracefully
            if isinstance(system_metrics, Exception):
                logger.error(f"System metrics collection failed: {system_metrics}")
                system_metrics = self._get_fallback_system_metrics()
            
            if isinstance(service_metrics, Exception):
                logger.error(f"Service metrics collection failed: {service_metrics}")
                service_metrics = []
            
            if isinstance(veris_metrics, Exception):
                logger.error(f"Veris metrics collection failed: {veris_metrics}")
                veris_metrics = self._get_fallback_veris_metrics()
            
            if isinstance(security_metrics, Exception):
                logger.error(f"Security metrics collection failed: {security_metrics}")
                security_metrics = self._get_fallback_security_metrics()
            
            if isinstance(backup_metrics, Exception):
                logger.error(f"Backup metrics collection failed: {backup_metrics}")
                backup_metrics = self._get_fallback_backup_metrics()

            # Compile all metrics
            all_metrics = {
                'timestamp': datetime.utcnow().isoformat(),
                'system': asdict(system_metrics),
                'services': [asdict(s) for s in service_metrics],
                'veris': asdict(veris_metrics),
                'security': asdict(security_metrics),
                'backups': asdict(backup_metrics),
            }

            # Update cache
            self.cached_metrics = all_metrics
            self.last_update = datetime.utcnow()

            return all_metrics

        except Exception as e:
            logger.error(f"Failed to collect metrics: {e}")
            return self._get_fallback_metrics()

    async def _collect_system_metrics(self) -> SystemMetrics:
        """Collect system-level metrics using existing MetricsCollector."""
        try:
            # Get metrics from the existing MetricsCollector time series
            cpu_stats = self.metrics_collector.get_metric_stats("system_cpu", 1)
            memory_stats = self.metrics_collector.get_metric_stats("system_memory", 1) 
            disk_stats = self.metrics_collector.get_metric_stats("system_disk", 1)
            
            # Use collected metrics if available, fallback to direct collection
            cpu_percent = cpu_stats.get("avg", 0) if cpu_stats.get("count", 0) > 0 else self._get_direct_cpu()
            memory_percent = memory_stats.get("avg", 0) if memory_stats.get("count", 0) > 0 else self._get_direct_memory()
            disk_percent = disk_stats.get("avg", 0) if disk_stats.get("count", 0) > 0 else self._get_direct_disk()
            
            # Get additional system details that MetricsCollector doesn't track
            memory_total_gb, memory_used_gb, load_average, uptime_hours = self._get_system_details()
            disk_total_gb = self._get_disk_total_gb()
            disk_used_gb = (disk_total_gb * disk_percent) / 100

            return SystemMetrics(
                cpu_percent=round(cpu_percent, 1),
                memory_total_gb=round(memory_total_gb, 1),
                memory_used_gb=round(memory_used_gb, 1),
                memory_percent=round(memory_percent, 1),
                disk_total_gb=round(disk_total_gb, 1),
                disk_used_gb=round(disk_used_gb, 1),
                disk_percent=round(disk_percent, 1),
                load_average=load_average,
                uptime_hours=round(uptime_hours, 1)
            )

        except Exception as e:
            logger.warning(f"Error collecting system metrics from MetricsCollector: {e}")
            return self._get_fallback_system_metrics()

    def _get_direct_cpu(self) -> float:
        """Get CPU usage directly when MetricsCollector data is unavailable."""
        if HAS_PSUTIL:
            try:
                return psutil.cpu_percent(interval=0.1)  # Shorter interval to avoid hanging
            except Exception as e:
                logger.warning(f"Failed to get CPU usage: {e}")
        
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return fallback.get('cpu_percent', 0.0)

    def _get_direct_memory(self) -> float:
        """Get memory usage directly when MetricsCollector data is unavailable."""
        if HAS_PSUTIL:
            try:
                return psutil.virtual_memory().percent
            except Exception as e:
                logger.warning(f"Failed to get memory usage: {e}")
        
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return fallback.get('memory_percent', 0.0)

    def _get_direct_disk(self) -> float:
        """Get disk usage directly when MetricsCollector data is unavailable."""
        if HAS_PSUTIL:
            try:
                disk = psutil.disk_usage('/')
                return (disk.used / disk.total) * 100
            except Exception as e:
                logger.warning(f"Failed to get disk usage: {e}")
        
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return fallback.get('disk_percent', 0.0)

    def _get_system_details(self) -> Tuple[float, float, List[float], float]:
        """Get detailed system information not tracked by MetricsCollector."""
        if HAS_PSUTIL:
            try:
                # Memory details
                memory = psutil.virtual_memory()
                memory_total_gb = memory.total / (1024**3)
                memory_used_gb = memory.used / (1024**3)
                
                # Load average
                load_average = list(psutil.getloadavg())
                
                # Uptime
                boot_time = psutil.boot_time()
                uptime_hours = (time.time() - boot_time) / 3600
                
                return memory_total_gb, memory_used_gb, load_average, uptime_hours
            except Exception as e:
                logger.warning(f"Failed to get system details: {e}")
        
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return (
            fallback.get('memory_total_gb', 64.0),
            fallback.get('memory_used_gb', 22.0),
            fallback.get('load_average', [0.1, 0.2, 0.3]),
            fallback.get('uptime_hours', 100.0)
        )

    def _get_disk_total_gb(self) -> float:
        """Get disk total capacity."""
        if HAS_PSUTIL:
            try:
                disk = psutil.disk_usage('/')
                return disk.total / (1024**3)
            except Exception as e:
                logger.warning(f"Failed to get disk capacity: {e}")
        
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return fallback.get('disk_total_gb', 500.0)

    async def _collect_service_metrics(self) -> List[ServiceMetrics]:
        """Collect service health and performance metrics with concurrent health checks."""
        services = []
        
        # MCP Server (self) - always healthy if we can collect
        services.append(ServiceMetrics(
            name="MCP Server", 
            status="healthy",
            port=8000
        ))
        
        # Run all external service health checks concurrently
        async def check_redis():
            """Check Redis health concurrently."""
            if not self.service_clients['redis']:
                return "unknown"
            try:
                await asyncio.get_event_loop().run_in_executor(
                    None, self.service_clients['redis'].ping
                )
                return "healthy"
            except ConnectionError as e:
                logger.debug(f"Redis connection failed: {e}")
                return "unhealthy"
            except TimeoutError as e:
                logger.debug(f"Redis health check timed out: {e}")
                return "unhealthy"
            except Exception as e:
                logger.debug(f"Redis health check failed with unexpected error: {e}")
                return "unhealthy"
        
        async def check_neo4j():
            """Check Neo4j health concurrently."""
            if not self.service_clients['neo4j']:
                return "unknown"
            try:
                result = await asyncio.get_event_loop().run_in_executor(
                    None, self.service_clients['neo4j'].query, "RETURN 1 as test"
                )
                return "healthy" if result else "unhealthy"
            except ConnectionError as e:
                logger.debug(f"Neo4j connection failed: {e}")
                return "unhealthy"
            except TimeoutError as e:
                logger.debug(f"Neo4j health check timed out: {e}")
                return "unhealthy"
            except ValueError as e:
                logger.debug(f"Neo4j authentication or query error: {e}")
                return "unhealthy"
            except Exception as e:
                logger.debug(f"Neo4j health check failed with unexpected error: {e}")
                return "unhealthy"
        
        async def check_qdrant():
            """Check Qdrant health concurrently."""
            if not self.service_clients['qdrant']:
                return "unknown"
            try:
                collections = await asyncio.get_event_loop().run_in_executor(
                    None, self.service_clients['qdrant'].get_collections
                )
                return "healthy" if collections else "unhealthy"
            except ConnectionError as e:
                logger.debug(f"Qdrant connection failed: {e}")
                return "unhealthy"
            except TimeoutError as e:
                logger.debug(f"Qdrant health check timed out: {e}")
                return "unhealthy"
            except ValueError as e:
                logger.debug(f"Qdrant API or authentication error: {e}")
                return "unhealthy"
            except Exception as e:
                logger.debug(f"Qdrant health check failed with unexpected error: {e}")
                return "unhealthy"
        
        # Execute all health checks concurrently with timeout
        try:
            redis_status, neo4j_status, qdrant_status = await asyncio.wait_for(
                asyncio.gather(check_redis(), check_neo4j(), check_qdrant()),
                timeout=10.0  # Total timeout for all health checks
            )
        except asyncio.TimeoutError:
            logger.warning("Service health checks timed out after 10 seconds")
            redis_status = neo4j_status = qdrant_status = "unhealthy"
        except Exception as e:
            logger.error(f"Unexpected error during concurrent health checks: {e}")
            redis_status = neo4j_status = qdrant_status = "unhealthy"
        
        # Add services with their health status
        services.extend([
            ServiceMetrics(name="Redis", status=redis_status, port=6379),
            ServiceMetrics(name="Neo4j HTTP", status=neo4j_status, port=7474),
            ServiceMetrics(name="Neo4j Bolt", status=neo4j_status, port=7687),
            ServiceMetrics(name="Qdrant", status=qdrant_status, port=6333),
        ])
        
        return services

    async def _get_qdrant_memory_count(self) -> int:
        """Get actual memory count from Qdrant collections."""
        total_vectors = 0
        
        if not self.service_clients['qdrant']:
            return 0
            
        try:
            # Get collections info
            collections_result = await asyncio.get_event_loop().run_in_executor(
                None, self.service_clients['qdrant'].get_collections
            )
            
            if hasattr(collections_result, 'collections'):
                for collection in collections_result.collections:
                    if hasattr(collection, 'name'):
                        # Get collection details
                        try:
                            collection_info = await asyncio.get_event_loop().run_in_executor(
                                None, self.service_clients['qdrant'].get_collection, collection.name
                            )
                            if hasattr(collection_info, 'vectors_count'):
                                total_vectors += collection_info.vectors_count or 0
                        except Exception as e:
                            logger.debug(f"Error getting collection {collection.name} info: {e}")
                            
        except Exception as e:
            logger.debug(f"Error getting Qdrant memory count: {e}")
            
        return total_vectors

    async def _get_active_agent_count(self) -> int:
        """Get actual active agent count from Redis scratchpad keys."""
        active_agents = 0
        
        if not self.service_clients['redis']:
            return 0
            
        try:
            # Get scratchpad keys to count unique agents
            keys = await asyncio.get_event_loop().run_in_executor(
                None, self.service_clients['redis'].keys, "scratchpad:*"
            )
            
            if keys:
                # Extract unique agent IDs from keys like "scratchpad:agent_id:key"
                agent_ids = set()
                for key in keys:
                    key_str = key.decode('utf-8') if isinstance(key, bytes) else str(key)
                    parts = key_str.split(':')
                    if len(parts) >= 2:
                        agent_ids.add(parts[1])
                active_agents = len(agent_ids)
                
        except Exception as e:
            logger.debug(f"Error getting active agent count: {e}")
            
        return active_agents

    async def _update_request_metrics(self) -> None:
        """Update metrics from the global request metrics collector."""
        try:
            # Try to import and get request metrics
            from .request_metrics import get_metrics_collector
            request_collector = get_metrics_collector()
            
            # Get global stats from request collector
            global_stats = await request_collector.get_global_stats()
            
            # Update our metrics collector with real data
            if global_stats.get('avg_duration_ms'):
                # Convert back to seconds for consistency
                self.metrics_collector.record_metric(
                    "request_duration_avg", 
                    global_stats['avg_duration_ms'] / 1000
                )
            
            if global_stats.get('p99_duration_ms'):
                self.metrics_collector.record_metric(
                    "request_duration_p99", 
                    global_stats['p99_duration_ms'] / 1000
                )
            
            if global_stats.get('error_rate_percent') is not None:
                self.metrics_collector.record_metric(
                    "error_rate_percent", 
                    global_stats['error_rate_percent']
                )
            
            # Record request counts as operations
            if global_stats.get('total_requests'):
                successful_requests = global_stats['total_requests'] - global_stats.get('total_errors', 0)
                self.metrics_collector.record_metric("successful_operations", successful_requests)
                self.metrics_collector.record_metric("failed_operations", global_stats.get('total_errors', 0))
                
        except Exception as e:
            logger.debug(f"Could not update request metrics: {e}")

    async def _collect_veris_metrics(self) -> VerisMetrics:
        """Collect real Veris Memory specific metrics from actual storage backends."""
        # Initialize default values
        total_memories = 0
        memories_today = 0
        avg_latency_ms = 0.0
        p99_latency_ms = 0.0
        error_rate_percent = 0.0
        active_agents = 0
        successful_ops = 0
        failed_ops = 0
        
        try:
            # Get actual memory count from Qdrant collections
            total_memories = await self._get_qdrant_memory_count()
            
            # Get actual active agents from Redis scratchpad keys
            active_agents = await self._get_active_agent_count()
            
            # Get real-time latency and error metrics from request collector
            await self._update_request_metrics()
            
            # Get latency metrics from request metrics collector
            latency_stats = self.metrics_collector.get_metric_stats("request_duration", 60)
            avg_latency_ms = latency_stats.get("avg", 0) * 1000 if latency_stats.get("avg") else 0.0
            p99_latency_ms = latency_stats.get("p99", 0) * 1000 if latency_stats.get("p99") else 0.0
            
            # Get error rate from request metrics
            error_rate_percent = self.metrics_collector.get_metric_value("error_rate_percent") or 0.0
            
            # Get operation counts from request metrics
            successful_ops = self.metrics_collector.get_metric_stats("successful_operations", 1440).get("sum", 0)
            failed_ops = self.metrics_collector.get_metric_stats("failed_operations", 1440).get("sum", 0)
            
            # Calculate daily memory growth (placeholder for now)
            memories_today = self.metrics_collector.get_metric_stats("memories_stored_today", 1440).get("sum", 0)
            
        except Exception as e:
            logger.error(f"Error collecting Veris metrics: {e}")
            # Use fallback values on error

        return VerisMetrics(
            total_memories=int(total_memories),
            memories_today=int(memories_today),
            avg_query_latency_ms=round(avg_latency_ms, 1),
            p99_latency_ms=round(p99_latency_ms, 1),
            error_rate_percent=round(error_rate_percent, 3),
            active_agents=int(active_agents),
            successful_operations_24h=int(successful_ops),
            failed_operations_24h=int(failed_ops)
        )

    async def _collect_security_metrics(self) -> SecurityMetrics:
        """Collect security and compliance metrics."""
        failed_auth = self.metrics_collector.get_metric_value("auth_failures") or 0
        blocked_ips = self.metrics_collector.get_metric_value("blocked_ips") or 0
        waf_blocks = self.metrics_collector.get_metric_stats("waf_blocked_requests", 1440).get("sum", 0)
        rbac_violations = self.metrics_collector.get_metric_value("rbac_violations") or 0
        audit_events = self.metrics_collector.get_metric_stats("audit_events", 1440).get("sum", 0)
        
        # SSL certificate expiry (mock - would need actual cert checking)
        ssl_expiry_days = 87  # Placeholder

        return SecurityMetrics(
            failed_auth_attempts=int(failed_auth),
            blocked_ips=int(blocked_ips),
            waf_blocks_today=int(waf_blocks),
            ssl_cert_expiry_days=ssl_expiry_days,
            rbac_violations=int(rbac_violations),
            audit_events_24h=int(audit_events)
        )

    async def _collect_backup_metrics(self) -> BackupMetrics:
        """Collect backup and disaster recovery metrics."""
        # These would integrate with actual backup system
        last_backup = datetime.utcnow() - timedelta(hours=3)  # Mock: 3 hours ago
        backup_size = 4.7  # GB
        restore_tested = True
        last_restore_time = 142.0  # seconds
        success_rate = 100.0
        sync_status = "healthy"

        return BackupMetrics(
            last_backup_time=last_backup,
            backup_size_gb=backup_size,
            restore_tested=restore_tested,
            last_restore_time_seconds=last_restore_time,
            backup_success_rate_percent=success_rate,
            offsite_sync_status=sync_status
        )


    def generate_json_dashboard(self, metrics: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate JSON format dashboard for agent consumption.
        
        Args:
            metrics: Pre-collected metrics (optional)
            
        Returns:
            JSON formatted dashboard string
        """
        if metrics is None:
            # This will be async in actual usage
            import asyncio
            metrics = asyncio.run(self.collect_all_metrics())

        if self.config['json']['pretty_print']:
            return json.dumps(metrics, indent=2, default=str)
        else:
            return json.dumps(metrics, default=str)

    def generate_ascii_dashboard(self, metrics: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate ASCII format dashboard for human operators.
        
        Args:
            metrics: Pre-collected metrics (optional)
            
        Returns:
            ASCII formatted dashboard string
        """
        if metrics is None:
            # This will be async in actual usage
            import asyncio
            metrics = asyncio.run(self.collect_all_metrics())

        # Import ASCII renderer
        from .ascii_renderer import ASCIIRenderer
        
        renderer = ASCIIRenderer(self.config['ascii'])
        return renderer.render_dashboard(metrics, self.config['thresholds'])

    async def generate_json_dashboard_with_analytics(self, metrics: Optional[Dict[str, Any]] = None, include_trends: bool = True, minutes: int = 5) -> str:
        """
        Generate enhanced JSON format dashboard for agent consumption with analytics.
        
        Args:
            metrics: Pre-collected metrics (optional)
            include_trends: Include trending data for analytics
            minutes: Minutes of trending data to include
            
        Returns:
            JSON formatted dashboard string with analytics
        """
        if metrics is None:
            metrics = await self.collect_all_metrics()
        
        # Enhanced metrics with analytics
        enhanced_metrics = dict(metrics)
        
        if include_trends and self.config['json'].get('include_trends', True):
            try:
                # Get request metrics analytics
                from .request_metrics import get_metrics_collector
                request_collector = get_metrics_collector()
                
                # Add trending data
                trending_data = await request_collector.get_trending_data(minutes)
                endpoint_stats = await request_collector.get_endpoint_stats()
                global_stats = await request_collector.get_global_stats()
                
                enhanced_metrics['analytics'] = {
                    'trending_data': trending_data,
                    'endpoint_statistics': endpoint_stats,
                    'global_request_stats': global_stats,
                    'analytics_window_minutes': minutes,
                    'last_analytics_update': datetime.utcnow().isoformat()
                }
                
                # Add performance insights
                enhanced_metrics['insights'] = await self._generate_performance_insights(global_stats, endpoint_stats)
                
            except Exception as e:
                logger.debug(f"Could not add analytics to JSON dashboard: {e}")
                enhanced_metrics['analytics'] = {
                    'error': 'Analytics data unavailable',
                    'reason': str(e)
                }

        if self.config['json']['pretty_print']:
            return json.dumps(enhanced_metrics, indent=2, default=str)
        else:
            return json.dumps(enhanced_metrics, default=str)

    async def _generate_performance_insights(self, global_stats: Dict[str, float], endpoint_stats: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """Generate performance insights for AI agent analysis."""
        insights = {
            'performance_status': 'good',
            'alerts': [],
            'recommendations': [],
            'key_metrics': {}
        }
        
        # Analyze global performance
        error_rate = global_stats.get('error_rate_percent', 0.0)
        avg_latency = global_stats.get('avg_duration_ms', 0.0)
        p99_latency = global_stats.get('p99_duration_ms', 0.0)
        
        # Performance thresholds from config
        thresholds = self.config.get('thresholds', {})
        error_warning = thresholds.get('error_rate_warning_percent', 1.0)
        error_critical = thresholds.get('error_rate_critical_percent', 5.0)
        latency_warning = thresholds.get('latency_warning_ms', 100)
        latency_critical = thresholds.get('latency_critical_ms', 500)
        
        # Error rate analysis
        if error_rate >= error_critical:
            insights['performance_status'] = 'critical'
            insights['alerts'].append({
                'type': 'error_rate_critical',
                'message': f'Error rate at {error_rate:.2f}% exceeds critical threshold ({error_critical}%)',
                'severity': 'critical'
            })
            insights['recommendations'].append('Investigate failing requests immediately')
        elif error_rate >= error_warning:
            insights['performance_status'] = 'warning'
            insights['alerts'].append({
                'type': 'error_rate_warning',
                'message': f'Error rate at {error_rate:.2f}% exceeds warning threshold ({error_warning}%)',
                'severity': 'warning'
            })
        
        # Latency analysis
        if p99_latency >= latency_critical:
            insights['performance_status'] = 'critical'
            insights['alerts'].append({
                'type': 'latency_critical',
                'message': f'P99 latency at {p99_latency:.1f}ms exceeds critical threshold ({latency_critical}ms)',
                'severity': 'critical'
            })
            insights['recommendations'].append('Optimize slow endpoints and database queries')
        elif avg_latency >= latency_warning:
            if insights['performance_status'] == 'good':
                insights['performance_status'] = 'warning'
            insights['alerts'].append({
                'type': 'latency_warning',
                'message': f'Average latency at {avg_latency:.1f}ms exceeds warning threshold ({latency_warning}ms)',
                'severity': 'warning'
            })
        
        # Endpoint-specific analysis
        slow_endpoints = []
        error_endpoints = []
        
        for endpoint, stats in endpoint_stats.items():
            if stats.get('error_rate_percent', 0) > error_warning:
                error_endpoints.append({
                    'endpoint': endpoint,
                    'error_rate': stats['error_rate_percent'],
                    'request_count': stats.get('request_count', 0)
                })
            
            if stats.get('avg_duration_ms', 0) > latency_warning:
                slow_endpoints.append({
                    'endpoint': endpoint,
                    'avg_latency_ms': stats['avg_duration_ms'],
                    'p99_latency_ms': stats.get('p99_duration_ms', 0)
                })
        
        if slow_endpoints:
            insights['slow_endpoints'] = sorted(slow_endpoints, key=lambda x: x['avg_latency_ms'], reverse=True)[:5]
            insights['recommendations'].append('Focus optimization on slowest endpoints')
        
        if error_endpoints:
            insights['error_endpoints'] = sorted(error_endpoints, key=lambda x: x['error_rate'], reverse=True)[:5]
            insights['recommendations'].append('Investigate endpoints with highest error rates')
        
        # Key metrics summary
        insights['key_metrics'] = {
            'requests_per_minute': global_stats.get('requests_per_minute', 0),
            'error_rate_percent': error_rate,
            'avg_latency_ms': avg_latency,
            'p99_latency_ms': p99_latency,
            'total_requests': global_stats.get('total_requests', 0),
            'unique_endpoints': len(endpoint_stats)
        }
        
        # Calculate overall performance score (0.0 - 1.0)
        performance_score = self._calculate_performance_score(error_rate, avg_latency, p99_latency, thresholds)
        insights['performance_score'] = performance_score
        
        return insights

    def _calculate_performance_score(self, error_rate: float, avg_latency: float, p99_latency: float, thresholds: Dict[str, Any]) -> float:
        """Calculate overall performance score based on error rate, latency, and throughput.
        
        Args:
            error_rate: Error rate percentage (0-100)
            avg_latency: Average latency in milliseconds
            p99_latency: P99 latency in milliseconds
            thresholds: Performance thresholds configuration
            
        Returns:
            Performance score between 0.0 (worst) and 1.0 (best)
        """
        # Get thresholds with defaults
        error_warning = thresholds.get('error_rate_warning_percent', 1.0)
        error_critical = thresholds.get('error_rate_critical_percent', 5.0)
        latency_warning = thresholds.get('latency_warning_ms', 100)
        latency_critical = thresholds.get('latency_critical_ms', 500)
        
        # Error rate score component (40% weight)
        if error_rate <= error_warning / 2:
            error_score = 1.0
        elif error_rate <= error_warning:
            error_score = 0.9 - (error_rate / error_warning * 0.1)
        elif error_rate <= error_critical:
            error_score = 0.7 - ((error_rate - error_warning) / (error_critical - error_warning) * 0.4)
        else:
            error_score = max(0.0, 0.3 - (error_rate - error_critical) / 10.0 * 0.3)
        
        # Average latency score component (35% weight)
        if avg_latency <= latency_warning / 2:
            avg_latency_score = 1.0
        elif avg_latency <= latency_warning:
            avg_latency_score = 0.9 - (avg_latency / latency_warning * 0.1)
        elif avg_latency <= latency_critical:
            avg_latency_score = 0.6 - ((avg_latency - latency_warning) / (latency_critical - latency_warning) * 0.3)
        else:
            avg_latency_score = max(0.0, 0.3 - (avg_latency - latency_critical) / latency_critical * 0.3)
        
        # P99 latency score component (25% weight)
        if p99_latency <= latency_critical / 2:
            p99_score = 1.0
        elif p99_latency <= latency_critical:
            p99_score = 0.8 - (p99_latency / latency_critical * 0.2)
        elif p99_latency <= latency_critical * 2:
            p99_score = 0.4 - ((p99_latency - latency_critical) / latency_critical * 0.2)
        else:
            p99_score = max(0.0, 0.2 - (p99_latency - latency_critical * 2) / (latency_critical * 2) * 0.2)
        
        # Calculate weighted performance score
        performance_score = (
            error_score * 0.40 +          # Error rate: 40% weight
            avg_latency_score * 0.35 +    # Average latency: 35% weight
            p99_score * 0.25              # P99 latency: 25% weight
        )
        
        # Ensure score is between 0.0 and 1.0
        return max(0.0, min(1.0, performance_score))

    def _get_fallback_system_metrics(self) -> SystemMetrics:
        """Get fallback system metrics when psutil is unavailable."""
        fallback = self.config.get('fallback_data', {}).get('system', {})
        return SystemMetrics(
            cpu_percent=fallback.get('cpu_percent', 0.0),
            memory_total_gb=fallback.get('memory_total_gb', 64.0),
            memory_used_gb=fallback.get('memory_used_gb', 22.0),
            memory_percent=fallback.get('memory_percent', 34.4),
            disk_total_gb=fallback.get('disk_total_gb', 436.0),
            disk_used_gb=fallback.get('disk_used_gb', 13.0),
            disk_percent=fallback.get('disk_percent', 3.0),
            load_average=fallback.get('load_average', [0.23, 0.24, 0.11]),
            uptime_hours=fallback.get('uptime_hours', 247.0)
        )

    def _get_fallback_veris_metrics(self) -> VerisMetrics:
        """Get fallback Veris metrics."""
        fallback = self.config.get('fallback_data', {}).get('veris', {})
        return VerisMetrics(
            total_memories=fallback.get('total_memories', 84532),
            memories_today=fallback.get('memories_today', 1247),
            avg_query_latency_ms=fallback.get('avg_query_latency_ms', 23.0),
            p99_latency_ms=fallback.get('p99_latency_ms', 89.0),
            error_rate_percent=fallback.get('error_rate_percent', 0.02),
            active_agents=fallback.get('active_agents', 12),
            successful_operations_24h=fallback.get('successful_operations_24h', 15420),
            failed_operations_24h=fallback.get('failed_operations_24h', 3)
        )

    def _get_fallback_security_metrics(self) -> SecurityMetrics:
        """Get fallback security metrics."""
        fallback = self.config.get('fallback_data', {}).get('security', {})
        return SecurityMetrics(
            failed_auth_attempts=fallback.get('failed_auth_attempts', 0),
            blocked_ips=fallback.get('blocked_ips', 2),
            waf_blocks_today=fallback.get('waf_blocks_today', 7),
            ssl_cert_expiry_days=fallback.get('ssl_cert_expiry_days', 87),
            rbac_violations=fallback.get('rbac_violations', 0),
            audit_events_24h=fallback.get('audit_events_24h', 245)
        )

    def _get_fallback_backup_metrics(self) -> BackupMetrics:
        """Get fallback backup metrics."""
        fallback = self.config.get('fallback_data', {}).get('backup', {})
        return BackupMetrics(
            last_backup_time=datetime.utcnow() - timedelta(hours=3),
            backup_size_gb=fallback.get('backup_size_gb', 4.7),
            restore_tested=fallback.get('restore_tested', True),
            last_restore_time_seconds=fallback.get('last_restore_time_seconds', 142.0),
            backup_success_rate_percent=fallback.get('backup_success_rate_percent', 100.0),
            offsite_sync_status=fallback.get('offsite_sync_status', 'healthy')
        )

    def _get_fallback_metrics(self) -> Dict[str, Any]:
        """Get complete fallback metrics."""
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'system': asdict(self._get_fallback_system_metrics()),
            'services': [],
            'veris': asdict(self._get_fallback_veris_metrics()),
            'security': asdict(self._get_fallback_security_metrics()),
            'backups': asdict(self._get_fallback_backup_metrics())
        }

    async def shutdown(self) -> None:
        """Clean shutdown of dashboard components."""
        self.metrics_collector.stop_collection()
        logger.info("UnifiedDashboard shutdown complete")


# Export main class
__all__ = ["UnifiedDashboard"]