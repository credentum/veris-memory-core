#!/usr/bin/env python3
"""
Test Phase 4 observability and safety features for enhanced fact recall.

This script tests the observability, telemetry, privacy controls, feature gates,
and monitoring components to validate Phase 4 implementation.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import time
import json
from unittest.mock import Mock, patch

from monitoring.fact_telemetry import FactTelemetry, FactOperation, TelemetryLevel, telemetry_context
from security.fact_privacy import (
    PrivacyEnforcer, DataClassifier, DataRedactor, PIIDetector, 
    DataClassification, RedactionLevel, PrivacyAwareFact
)
from core.feature_gates import (
    FeatureGateManager, FeatureState, RolloutStrategy, 
    is_feature_enabled, feature_gate
)
from monitoring.fact_monitoring import (
    FactMonitoringSystem, AlertManager, MetricCollector, DashboardManager,
    AlertSeverity, AlertCondition, record_operation_metric
)


def test_fact_telemetry():
    """Test comprehensive fact telemetry system."""
    print("ğŸ“Š Testing Fact Telemetry System")
    print("-" * 40)
    
    # Initialize telemetry with test configuration
    config = {
        'telemetry_level': 'detailed',
        'scrub_pii': True,
        'retention_hours': 1
    }
    
    telemetry = FactTelemetry(config)
    
    # Test operation tracing
    print("ğŸ” Testing operation tracing...")
    
    with telemetry_context(
        FactOperation.STORE_FACT, 
        "test_user", 
        "test_namespace",
        {"attribute": "email", "confidence": 0.95}
    ) as context:
        # Simulate work
        time.sleep(0.01)
        
        print(f"   Trace ID: {context.trace_id}")
        print(f"   Operation: {context.operation.value}")
        print(f"   User ID: {context.user_id}")
    
    # Test manual success recording
    trace_context = telemetry.trace_request(
        FactOperation.RETRIEVE_FACT,
        "user123",
        "test_namespace",
        {"query": "What's my email?"}
    )
    
    telemetry.record_success(trace_context, 45.5, {"results_count": 5})
    
    # Test failure recording
    trace_context2 = telemetry.trace_request(
        FactOperation.ENTITY_EXTRACTION,
        "user456",
        "test_namespace"
    )
    
    test_error = ValueError("Mock extraction error")
    telemetry.record_failure(trace_context2, 12.3, test_error)
    
    # Test routing decisions
    telemetry.log_routing_decision(
        "What's my work email?",
        "FACT_LOOKUP",
        "email_lookup_route",
        0.87,
        ["general_search", "semantic_lookup"],
        "High confidence fact lookup pattern detected"
    )
    
    # Test ranking explanations
    telemetry.record_ranking_explanation(
        "My email address",
        12,
        "FACT_OPTIMIZED",
        {"alpha_dense": 0.6, "beta_lexical": 0.3, "fact_boost": 0.1},
        [
            {"score": 0.95, "content": "email fact"},
            {"score": 0.87, "content": "contact info"}
        ],
        ["exact_attribute_match", "high_confidence_fact"]
    )
    
    # Test metrics retrieval
    recent_metrics = telemetry.get_operation_metrics(hours=1)
    failure_summary = telemetry.get_failure_summary(hours=1)
    performance_summary = telemetry.get_performance_summary(hours=1)
    
    print(f"âœ… Recent metrics: {len(recent_metrics)} operations")
    print(f"âœ… Failure summary: {failure_summary.get('total_failures', 0)} failures")
    print(f"âœ… Performance summary: {len(performance_summary.get('operation_stats', {}))} operation types")
    
    # Test telemetry export
    exported_data = telemetry.export_metrics("json")
    exported_dict = json.loads(exported_data)
    
    print(f"âœ… Exported telemetry data: {len(exported_dict)} sections")
    print(f"   Config level: {exported_dict['telemetry_config']['level']}")
    print(f"   Recent routing decisions: {len(exported_dict['recent_routing_decisions'])}")
    
    return len(recent_metrics) > 0 and failure_summary['total_failures'] > 0


def test_privacy_controls():
    """Test privacy and security controls for fact data."""
    print("\nğŸ”’ Testing Privacy and Security Controls")
    print("-" * 40)
    
    # Test PII detection
    print("ğŸ•µï¸ Testing PII detection...")
    
    pii_detector = PIIDetector()
    
    test_texts = [
        ("My email is alice@example.com", "email address"),
        ("Call me at 555-123-4567", "phone number"),
        ("My name is John Smith", "personal name"),
        ("I live at 123 Main Street", "home address"),
        ("My SSN is 123-45-6789", "social security"),
    ]
    
    detected_pii_total = 0
    for text, context in test_texts:
        detected = pii_detector.detect_pii(text, context)
        detected_pii_total += len(detected)
        print(f"   '{text}' -> {[p.value for p in detected]}")
    
    print(f"âœ… Total PII types detected: {detected_pii_total}")
    
    # Test data classification
    print("\nğŸ·ï¸ Testing data classification...")
    
    classifier = DataClassifier()
    
    test_facts = [
        ("email", "alice@company.com", "work contact"),
        ("ssn", "123-45-6789", "government ID"),
        ("name", "John Doe", "personal identification"),
        ("preference.food", "spicy cuisine", "personal preference"),
        ("location", "New York", "geographic location")
    ]
    
    classification_results = []
    for attribute, value, context in test_facts:
        result = classifier.classify_fact(attribute, value, context)
        classification_results.append(result)
        print(f"   {attribute}: {value} -> {result.classification.value} (confidence: {result.confidence:.2f})")
        print(f"      Reasoning: {result.reasoning}")
    
    print(f"âœ… Classifications generated: {len(classification_results)}")
    
    # Test data redaction
    print("\nğŸ­ Testing data redaction...")
    
    redactor = DataRedactor(salt="test_salt")
    
    test_redactions = [
        ("Contact me at alice@example.com", RedactionLevel.PARTIAL),
        ("My phone is 555-123-4567", RedactionLevel.FULL),
        ("John Smith works here", RedactionLevel.HASH),
        ("Credit card: 4532-1234-5678-9012", RedactionLevel.PARTIAL)
    ]
    
    redacted_texts = []
    for text, level in test_redactions:
        redacted = redactor.redact_data(text, level)
        redacted_texts.append(redacted)
        print(f"   {level.value}: '{text}' -> '{redacted}'")
    
    print(f"âœ… Redacted texts: {len(redacted_texts)}")
    
    # Test privacy enforcer
    print("\nğŸ›¡ï¸ Testing privacy enforcer...")
    
    enforcer = PrivacyEnforcer({'audit_all_access': True})
    
    # Test privacy policy application
    test_value = "john.doe@company.com"
    processed_value, policy = enforcer.apply_privacy_policy("email", test_value)
    
    print(f"   Original: {test_value}")
    print(f"   Processed: {processed_value}")
    print(f"   Policy classification: {policy.data_classification.value}")
    print(f"   Policy redaction: {policy.redaction_level.value}")
    
    # Test access permission checking
    user_context = {"location": "US", "fact_age_days": 30}
    permitted, reason = enforcer.check_access_permission(
        "read", "email", test_value, user_context, policy
    )
    
    print(f"   Access permitted: {permitted}, Reason: {reason}")
    
    # Test audit logging
    enforcer.log_access(
        "user123", "read", "email", 
        DataClassification.CONFIDENTIAL, permitted, reason
    )
    
    audit_summary = enforcer.get_audit_summary(hours=1)
    print(f"âœ… Audit entries: {audit_summary.get('total_accesses', 0)}")
    
    # Test privacy-aware fact wrapper
    print("\nğŸ” Testing privacy-aware fact wrapper...")
    
    fact = PrivacyAwareFact("email", "sensitive@example.com", enforcer)
    
    print(f"   Fact classification: {fact.classification.value}")
    print(f"   Processed value: {fact.value}")
    
    # Test access with permission
    access_granted, value, reason = fact.access_with_permission("read", "user123")
    print(f"   Access granted: {access_granted}, Value: {value}, Reason: {reason}")
    
    metadata = fact.get_metadata()
    print(f"   Metadata keys: {list(metadata.keys())}")
    
    return detected_pii_total > 0 and len(classification_results) > 0


def test_feature_gates():
    """Test feature gates and rollout infrastructure."""
    print("\nğŸšª Testing Feature Gates and Rollout")
    print("-" * 40)
    
    # Initialize feature gate manager
    config = {
        'testing_cohorts': ['dev', 'qa', 'beta'],
        'cache_ttl_seconds': 60
    }
    
    manager = FeatureGateManager(config)
    
    # Test default features
    print("ğŸ Testing default features...")
    
    default_features = [
        'graph_entity_extraction',
        'hybrid_scoring', 
        'query_expansion',
        'fact_telemetry',
        'privacy_controls'
    ]
    
    enabled_features = 0
    for feature in default_features:
        enabled = manager.is_enabled(feature, "test_user")
        if enabled:
            enabled_features += 1
        print(f"   {feature}: {'âœ… enabled' if enabled else 'âŒ disabled'}")
    
    print(f"âœ… Default features enabled: {enabled_features}/{len(default_features)}")
    
    # Test creating custom feature
    print("\nğŸ†• Testing custom feature creation...")
    
    manager.create_feature(
        "test_feature",
        state=FeatureState.ROLLOUT,
        rollout_percentage=50.0,
        rollout_strategy=RolloutStrategy.PERCENTAGE,
        metadata={"description": "Test rollout feature"}
    )
    
    # Test rollout behavior with different users
    rollout_results = {}
    test_users = [f"user{i:03d}" for i in range(100)]
    
    for user in test_users[:20]:  # Test with 20 users
        enabled = manager.is_enabled("test_feature", user)
        rollout_results[user] = enabled
    
    enabled_count = sum(rollout_results.values())
    rollout_percentage = (enabled_count / len(rollout_results)) * 100
    
    print(f"   Test rollout: {enabled_count}/{len(rollout_results)} users enabled ({rollout_percentage:.1f}%)")
    
    # Test cohort-based features
    print("\nğŸ‘¥ Testing cohort-based features...")
    
    manager.create_feature("beta_feature", state=FeatureState.TESTING)
    
    # Test with different cohorts
    cohort_tests = [
        ("regular_user", None, False),
        ("beta_user", "beta", True),
        ("dev_user", "dev", True),
        ("prod_user", "production", False)
    ]
    
    cohort_success = 0
    for user_id, cohort, expected in cohort_tests:
        enabled = manager.is_enabled("beta_feature", user_id, user_cohort=cohort)
        if enabled == expected:
            cohort_success += 1
        print(f"   {user_id} (cohort: {cohort}): {'âœ…' if enabled == expected else 'âŒ'} {'enabled' if enabled else 'disabled'}")
    
    print(f"âœ… Cohort tests passed: {cohort_success}/{len(cohort_tests)}")
    
    # Test feature gate context manager
    print("\nğŸ”„ Testing feature gate context manager...")
    
    with feature_gate("test_feature", "context_user", fallback_enabled=False) as enabled:
        if enabled:
            print("   âœ… Feature enabled - executing feature code")
        else:
            print("   âŒ Feature disabled - using fallback")
    
    # Test A/B testing
    print("\nğŸ§ª Testing A/B testing...")
    
    manager.create_ab_test(
        "email_search_test",
        "advanced_email_search",
        {"control": 50.0, "variant_a": 30.0, "variant_b": 20.0},
        "control",
        "click_through_rate",
        minimum_sample_size=100,
        duration_hours=168  # 1 week
    )
    
    # Test variant assignment consistency
    test_user = "ab_test_user"
    variants = []
    for _ in range(5):  # Multiple calls should return same variant
        variant = manager.get_ab_test_variant("email_search_test", test_user)
        variants.append(variant)
    
    consistent_variants = len(set(variants)) == 1
    print(f"   AB test consistency: {'âœ…' if consistent_variants else 'âŒ'} (variant: {variants[0]})")
    
    # Test feature statistics
    stats = manager.get_feature_stats(hours=1)
    print(f"âœ… Feature stats: {stats.get('total_evaluations', 0)} evaluations")
    
    return enabled_features > 0 and cohort_success == len(cohort_tests) and consistent_variants


def test_monitoring_system():
    """Test comprehensive monitoring and alerting system."""
    print("\nğŸ“ˆ Testing Monitoring and Alerting System")
    print("-" * 40)
    
    # Initialize monitoring system
    config = {
        'retention_hours': 1,
        'monitoring_interval': 1,
        'max_evaluations_log': 1000
    }
    
    monitoring = FactMonitoringSystem(config)
    
    # Test metric collection
    print("ğŸ“Š Testing metric collection...")
    
    # Record various metrics
    test_metrics = [
        ("fact_operations_total", 1.0, {"operation": "store_fact", "status": "success"}),
        ("fact_operation_duration_ms", 45.5, {"operation": "store_fact"}),
        ("fact_operations_total", 1.0, {"operation": "retrieve_fact", "status": "success"}),
        ("fact_operation_duration_ms", 123.7, {"operation": "retrieve_fact"}),
        ("fact_operation_errors_total", 1.0, {"operation": "retrieve_fact", "error_type": "timeout"}),
    ]
    
    for metric_name, value, labels in test_metrics:
        monitoring.metric_collector.record_metric(metric_name, value, labels)
    
    # Test metric retrieval
    all_metrics = monitoring.metric_collector.get_all_metric_names()
    print(f"   Recorded metrics: {len(all_metrics)} unique metric names")
    
    # Test metric statistics
    for metric_name in all_metrics[:3]:  # Test first 3 metrics
        stats = monitoring.metric_collector.get_metric_stats(metric_name, hours=1.0)
        if "error" not in stats:
            print(f"   {metric_name}: count={stats['count']}, mean={stats['mean']:.2f}")
    
    # Test alert management
    print("\nğŸš¨ Testing alert management...")
    
    # Trigger alert by recording high error rate
    for _ in range(15):  # Trigger high error rate alert
        monitoring.metric_collector.record_metric(
            "fact_operation_errors_total", 1.0, {"operation": "test", "error_type": "mock"}
        )
    
    # Evaluate alerts
    fired_alerts = monitoring.alert_manager.evaluate_alerts()
    active_alerts = monitoring.alert_manager.get_active_alerts()
    
    print(f"   Fired alerts: {len(fired_alerts)}")
    print(f"   Active alerts: {len(active_alerts)}")
    
    for alert in active_alerts:
        print(f"   Alert: {alert.rule_name} ({alert.severity.value}) - {alert.description}")
    
    # Test dashboard management
    print("\nğŸ“‹ Testing dashboard management...")
    
    dashboards = monitoring.dashboard_manager.list_dashboards()
    print(f"   Available dashboards: {len(dashboards)}")
    
    for dashboard in dashboards:
        print(f"   - {dashboard['title']} ({dashboard['widget_count']} widgets)")
    
    # Test dashboard data retrieval
    if dashboards:
        dashboard_id = dashboards[0]['dashboard_id']
        dashboard_data = monitoring.dashboard_manager.get_dashboard_data(dashboard_id)
        
        if "error" not in dashboard_data:
            widgets_count = len(dashboard_data.get('widgets_data', {}))
            print(f"   Dashboard data loaded: {widgets_count} widgets with data")
    
    # Test system health
    print("\nğŸ’š Testing system health checks...")
    
    # Add mock health check
    def mock_health_check():
        return True
    
    monitoring.add_health_check("test_component", mock_health_check)
    
    # Get system status
    system_status = monitoring.get_system_status()
    
    print(f"   System health: {system_status['system_health']['status']}")
    print(f"   Active alerts: {system_status['active_alerts']}")
    print(f"   Total metrics: {system_status['total_metrics']}")
    print(f"   Monitoring enabled: {system_status['monitoring_enabled']}")
    
    # Test convenience functions
    print("\nâš¡ Testing convenience functions...")
    
    # Record operation metrics
    record_operation_metric("test_operation", 67.8, True, user_id="test_user")
    record_operation_metric("test_operation", 156.3, False, user_id="test_user", error="timeout")
    
    print("   âœ… Operation metrics recorded")
    
    # Test monitoring configuration export
    config_export = monitoring.export_monitoring_config()
    
    print(f"   Configuration export: {len(config_export)} sections")
    print(f"   Alert rules: {len(config_export.get('alert_rules', {}))}")
    print(f"   Dashboards: {len(config_export.get('dashboards', {}))}")
    
    return len(all_metrics) > 0 and len(dashboards) > 0


def test_integration_workflow():
    """Test Phase 4 integration workflow with all components."""
    print("\nğŸ”— Testing Phase 4 Integration Workflow")
    print("-" * 40)
    
    # Initialize all Phase 4 components
    telemetry = FactTelemetry({'telemetry_level': 'detailed'})
    privacy_enforcer = PrivacyEnforcer()
    feature_manager = FeatureGateManager()
    monitoring = FactMonitoringSystem()
    
    print("ğŸš€ Simulating comprehensive fact operation with Phase 4 features...")
    
    # Simulate fact storage with full Phase 4 pipeline
    user_id = "integration_user"
    fact_attribute = "email"
    fact_value = "user@company.com"
    
    # Step 1: Feature gate check
    telemetry_enabled = feature_manager.is_enabled("fact_telemetry", user_id)
    privacy_enabled = feature_manager.is_enabled("privacy_controls", user_id)
    
    print(f"1. Feature gates: Telemetry={telemetry_enabled}, Privacy={privacy_enabled}")
    
    # Step 2: Privacy processing
    if privacy_enabled:
        privacy_fact = PrivacyAwareFact(fact_attribute, fact_value, privacy_enforcer)
        processed_value = privacy_fact.value
        classification = privacy_fact.classification
        
        print(f"2. Privacy processing: {fact_value} -> {processed_value} ({classification.value})")
    else:
        processed_value = fact_value
        classification = None
    
    # Step 3: Telemetry tracing
    if telemetry_enabled:
        with telemetry_context(
            FactOperation.STORE_FACT,
            user_id,
            "integration_test",
            {
                "attribute": fact_attribute,
                "classification": classification.value if classification else "public",
                "privacy_processed": privacy_enabled
            }
        ) as trace_context:
            # Simulate fact storage work
            time.sleep(0.02)
            
            print(f"3. Telemetry: Traced operation {trace_context.trace_id}")
        
        # Record additional metrics outside context for tracking
        monitoring.metric_collector.record_metric(
            "fact_storage_with_privacy", 1.0,
            {"classification": classification.value if classification else "public"}
        )
    
    # Step 4: Simulate fact retrieval with monitoring
    retrieval_enabled = feature_manager.is_enabled("hybrid_scoring", user_id)
    
    start_time = time.time()
    
    if retrieval_enabled:
        # Simulate retrieval work
        time.sleep(0.03)
        
        # Record retrieval metrics
        duration_ms = (time.time() - start_time) * 1000
        record_operation_metric("integrated_retrieval", duration_ms, True, 
                               user_id=user_id, features_enabled=3)
        
        print(f"4. Fact retrieval: {duration_ms:.1f}ms with enhanced features")
    
    # Step 5: Check for alerts
    alerts = monitoring.alert_manager.evaluate_alerts()
    
    print(f"5. Alert evaluation: {len(alerts)} alerts triggered")
    
    # Step 6: Generate integration metrics
    integration_stats = {
        "telemetry_traces": len(telemetry.get_operation_metrics(hours=1)),
        "privacy_classifications": 1 if privacy_enabled else 0,
        "feature_evaluations": 3,  # 3 features checked
        "monitoring_metrics": len(monitoring.metric_collector.get_all_metric_names()),
        "active_alerts": len(monitoring.alert_manager.get_active_alerts())
    }
    
    print(f"6. Integration metrics:")
    for metric, value in integration_stats.items():
        print(f"   {metric}: {value}")
    
    # Step 7: Export comprehensive status
    system_status = monitoring.get_system_status()
    privacy_report = privacy_enforcer.export_privacy_report()
    feature_stats = feature_manager.get_feature_stats()
    
    comprehensive_status = {
        "system_health": system_status["system_health"]["status"],
        "privacy_compliant": len(privacy_report["audit_summary_24h"]) >= 0,
        "features_operational": feature_stats["active_features"] > 0,
        "telemetry_active": len(telemetry.get_operation_metrics(hours=1)) > 0,
        "monitoring_active": system_status["monitoring_enabled"]
    }
    
    print(f"7. Phase 4 status: {comprehensive_status}")
    
    # Integration success criteria (realistic for isolated test instances)
    success_criteria = {
        "feature_gates_working": telemetry_enabled and privacy_enabled,
        "privacy_controls_active": privacy_enabled and classification is not None,
        "telemetry_functional": telemetry_enabled,  # Telemetry system is functional
        "monitoring_functional": len(monitoring.metric_collector.get_all_metric_names()) > 0,
        "system_integration": comprehensive_status["system_health"] == "healthy" and comprehensive_status["monitoring_active"]
    }
    
    print(f"\nğŸ¯ Integration success criteria:")
    for criterion, success in success_criteria.items():
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {criterion.replace('_', ' ').title()}: {success}")
    
    overall_success = all(success_criteria.values())
    print(f"\n{'ğŸ‰' if overall_success else 'âš ï¸'} Overall Phase 4 integration: {'SUCCESS' if overall_success else 'PARTIAL'}")
    
    return overall_success


def main():
    """Run all Phase 4 observability and safety tests."""
    print("ğŸ§ª Testing Phase 4 Observability and Safety Features")
    print("=" * 70)
    
    tests = [
        ("Fact Telemetry System", test_fact_telemetry),
        ("Privacy and Security Controls", test_privacy_controls),
        ("Feature Gates and Rollout", test_feature_gates),
        ("Monitoring and Alerting", test_monitoring_system),
        ("Phase 4 Integration Workflow", test_integration_workflow)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
            status = "âœ… PASS" if result else "âŒ FAIL"
            print(f"\n{status}: {test_name}")
        except Exception as e:
            results.append((test_name, False))
            print(f"\nâŒ ERROR: {test_name} - {e}")
            import traceback
            traceback.print_exc()
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“‹ Phase 4 Observability and Safety Test Results")
    print("-" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {status}: {test_name}")
    
    print(f"\nğŸ¯ Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All Phase 4 observability and safety components working correctly!")
        print("Ready for production deployment with:")
        print("  â€¢ Comprehensive telemetry and tracing")
        print("  â€¢ Privacy-aware fact processing")
        print("  â€¢ Sophisticated feature gates and rollout")
        print("  â€¢ Real-time monitoring and alerting")
        print("  â€¢ Complete observability and safety integration")
    else:
        print("âš ï¸  Some Phase 4 components need attention")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)